{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5214274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: / \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/noarch::aioitertools==0.7.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::importlib-metadata==3.7.0=py36h5fab9bb_0\n",
      "  - conda-forge/noarch::typing-extensions==3.7.4.3=0\n",
      "  - conda-forge/noarch::black==20.8b1=py_1\n",
      "  - conda-forge/noarch::helpdev==0.7.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::imageio==2.9.0=py_0\n",
      "  - conda-forge/noarch::importlib_metadata==3.7.0=hd8ed1ab_0\n",
      "  - conda-forge/linux-64::yarl==1.6.3=py36h8f6f2f9_1\n",
      "  - conda-forge/noarch::flake8==3.8.4=py_0\n",
      "  - conda-forge/noarch::jsonschema==3.2.0=py_2\n",
      "  - conda-forge/linux-64::matplotlib-base==3.3.4=py36hd391965_0\n",
      "  - conda-forge/linux-64::path==15.1.2=py36h5fab9bb_0\n",
      "  - conda-forge/linux-64::pluggy==0.13.1=py36h5fab9bb_4\n",
      "  - conda-forge/noarch::qdarkstyle==2.8.1=pyhd8ed1ab_2\n",
      "  - conda-forge/linux-64::anyio==2.1.0=py36h5fab9bb_0\n",
      "  - conda-forge/noarch::flask==1.1.2=pyh9f0ad1d_0\n",
      "  - conda-forge/linux-64::keyring==22.0.1=py36h5fab9bb_0\n",
      "  - conda-forge/noarch::nbformat==5.1.2=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::path.py==12.5.0=0\n",
      "  - conda-forge/linux-64::pytest==6.2.2=py36h5fab9bb_0\n",
      "  - conda-forge/linux-64::scikit-image==0.16.2=py36hb3f55d8_0\n",
      "  - conda-forge/noarch::seaborn-base==0.11.1=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::sphinx==3.5.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::anaconda-client==1.7.2=py_0\n",
      "  - conda-forge/noarch::dask==2021.2.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbclient==0.5.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::numpydoc==1.1.0=py_1\n",
      "  - conda-forge/noarch::python-language-server==0.36.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::seaborn==0.11.1=hd8ed1ab_1\n",
      "  - conda-forge/noarch::anaconda-project==0.9.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib==3.3.4=py36h5fab9bb_0\n",
      "  - conda-forge/linux-64::nbconvert==6.0.7=py36h5fab9bb_3\n",
      "  - conda-forge/noarch::odo==0.5.1=py_1\n",
      "  - conda-forge/noarch::pyls-black==0.4.6=pyh9f0ad1d_0\n",
      "  - conda-forge/noarch::pyls-spyder==0.3.2=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::blaze==0.11.3=py36_0\n",
      "  - conda-forge/linux-64::jupyter_server==1.4.1=py36h5fab9bb_0\n",
      "  - conda-forge/noarch::jupyterlab_server==2.3.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::jupyterlab_launcher==0.13.1=py_2\n",
      "  - conda-forge/linux-64::nb_conda==2.2.1=py36h5fab9bb_4\n",
      "  - conda-forge/noarch::nbclassic==0.2.6=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::spyder==4.2.0=py36h5fab9bb_0\n",
      "  - conda-forge/linux-64::widgetsnbextension==3.5.1=py36h5fab9bb_4\n",
      "  - conda-forge/noarch::ipywidgets==7.6.3=pyhd3deb0d_0\n",
      "  - conda-forge/linux-64::jupyter==1.0.0=py36h5fab9bb_6\n",
      "  - defaults/linux-64::_anaconda_depends==5.1.0=py36_2\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pandasql\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    astroid-2.5.8              |   py36h5fab9bb_0         316 KB  conda-forge\n",
      "    babel-2.8.0                |             py_0         6.0 MB  anaconda\n",
      "    bleach-3.2.1               |             py_0         111 KB  anaconda\n",
      "    bokeh-2.2.3                |           py36_0         7.0 MB  anaconda\n",
      "    ca-certificates-2020.10.14 |                0         128 KB  anaconda\n",
      "    certifi-2020.6.20          |           py36_0         160 KB  anaconda\n",
      "    charset-normalizer-2.0.10  |     pyhd8ed1ab_0          34 KB  conda-forge\n",
      "    colorama-0.4.4             |             py_0          18 KB  anaconda\n",
      "    dataclasses-0.7            |           py36_0          31 KB  anaconda\n",
      "    docutils-0.16              |           py36_1         740 KB  anaconda\n",
      "    flask-cors-3.0.9           |             py_0          18 KB  anaconda\n",
      "    fsspec-0.8.3               |             py_0          69 KB  anaconda\n",
      "    jupyter_console-5.2.0      |           py36_1          34 KB  conda-forge\n",
      "    jupyterlab-3.2.8           |     pyhd8ed1ab_0         5.8 MB  conda-forge\n",
      "    lxml-3.8.0                 |           py36_0         3.8 MB  conda-forge\n",
      "    nest-asyncio-1.4.1         |             py_0          10 KB  anaconda\n",
      "    notebook-6.1.4             |           py36_0         6.3 MB  anaconda\n",
      "    packaging-20.4             |             py_0          35 KB  anaconda\n",
      "    pandasql-0.7.3             |           py36_1          31 KB  anaconda\n",
      "    pillow-8.0.0               |   py36h9a89aac_0         675 KB  anaconda\n",
      "    pip-20.2.4                 |           py36_0         2.0 MB  anaconda\n",
      "    pylint-2.7.2               |   py36h5fab9bb_0         466 KB  conda-forge\n",
      "    requests-2.27.1            |     pyhd8ed1ab_0          53 KB  conda-forge\n",
      "    send2trash-1.5.0           |           py36_0          16 KB  anaconda\n",
      "    typing_extensions-3.7.4.3  |             py_0          29 KB  anaconda\n",
      "    urllib3-1.25.11            |             py_0          93 KB  anaconda\n",
      "    werkzeug-1.0.1             |             py_0         243 KB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        34.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  astroid            conda-forge/linux-64::astroid-2.5.8-py36h5fab9bb_0\n",
      "  babel              anaconda/noarch::babel-2.8.0-py_0\n",
      "  bleach             anaconda/noarch::bleach-3.2.1-py_0\n",
      "  bokeh              anaconda/linux-64::bokeh-2.2.3-py36_0\n",
      "  charset-normalizer conda-forge/noarch::charset-normalizer-2.0.10-pyhd8ed1ab_0\n",
      "  colorama           anaconda/noarch::colorama-0.4.4-py_0\n",
      "  dataclasses        anaconda/linux-64::dataclasses-0.7-py36_0\n",
      "  docutils           anaconda/linux-64::docutils-0.16-py36_1\n",
      "  flask-cors         anaconda/noarch::flask-cors-3.0.9-py_0\n",
      "  fsspec             anaconda/noarch::fsspec-0.8.3-py_0\n",
      "  jupyter_console    conda-forge/linux-64::jupyter_console-5.2.0-py36_1\n",
      "  jupyterlab         conda-forge/noarch::jupyterlab-3.2.8-pyhd8ed1ab_0\n",
      "  lxml               conda-forge/linux-64::lxml-3.8.0-py36_0\n",
      "  nest-asyncio       anaconda/noarch::nest-asyncio-1.4.1-py_0\n",
      "  notebook           anaconda/linux-64::notebook-6.1.4-py36_0\n",
      "  packaging          anaconda/noarch::packaging-20.4-py_0\n",
      "  pandasql           anaconda/linux-64::pandasql-0.7.3-py36_1\n",
      "  pillow             anaconda/linux-64::pillow-8.0.0-py36h9a89aac_0\n",
      "  pip                anaconda/linux-64::pip-20.2.4-py36_0\n",
      "  pylint             conda-forge/linux-64::pylint-2.7.2-py36h5fab9bb_0\n",
      "  requests           conda-forge/noarch::requests-2.27.1-pyhd8ed1ab_0\n",
      "  send2trash         anaconda/linux-64::send2trash-1.5.0-py36_0\n",
      "  typing_extensions  anaconda/noarch::typing_extensions-3.7.4.3-py_0\n",
      "  urllib3            anaconda/noarch::urllib3-1.25.11-py_0\n",
      "  werkzeug           anaconda/noarch::werkzeug-1.0.1-py_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2021.10.~ --> anaconda::ca-certificates-2020.10.14-0\n",
      "  certifi            conda-forge::certifi-2021.5.30-py36h5~ --> anaconda::certifi-2020.6.20-py36_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "requests-2.27.1      | 53 KB     | ##################################### | 100% \n",
      "flask-cors-3.0.9     | 18 KB     | ##################################### | 100% \n",
      "colorama-0.4.4       | 18 KB     | ##################################### | 100% \n",
      "pylint-2.7.2         | 466 KB    | ##################################### | 100% \n",
      "fsspec-0.8.3         | 69 KB     | ##################################### | 100% \n",
      "jupyterlab-3.2.8     | 5.8 MB    | ##################################### | 100% \n",
      "jupyter_console-5.2. | 34 KB     | ##################################### | 100% \n",
      "babel-2.8.0          | 6.0 MB    | ##################################### | 100% \n",
      "werkzeug-1.0.1       | 243 KB    | ##################################### | 100% \n",
      "pip-20.2.4           | 2.0 MB    | ##################################### | 100% \n",
      "send2trash-1.5.0     | 16 KB     | ##################################### | 100% \n",
      "notebook-6.1.4       | 6.3 MB    | ##################################### | 100% \n",
      "pandasql-0.7.3       | 31 KB     | ##################################### | 100% \n",
      "astroid-2.5.8        | 316 KB    | ##################################### | 100% \n",
      "typing_extensions-3. | 29 KB     | ##################################### | 100% \n",
      "bleach-3.2.1         | 111 KB    | ##################################### | 100% \n",
      "dataclasses-0.7      | 31 KB     | ##################################### | 100% \n",
      "packaging-20.4       | 35 KB     | ##################################### | 100% \n",
      "bokeh-2.2.3          | 7.0 MB    | ##################################### | 100% \n",
      "nest-asyncio-1.4.1   | 10 KB     | ##################################### | 100% \n",
      "lxml-3.8.0           | 3.8 MB    | ##################################### | 100% \n",
      "urllib3-1.25.11      | 93 KB     | ##################################### | 100% \n",
      "certifi-2020.6.20    | 160 KB    | ##################################### | 100% \n",
      "ca-certificates-2020 | 128 KB    | ##################################### | 100% \n",
      "docutils-0.16        | 740 KB    | ##################################### | 100% \n",
      "charset-normalizer-2 | 34 KB     | ##################################### | 100% \n",
      "pillow-8.0.0         | 675 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c anaconda pandasql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485e89fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs:\n",
      "    - openpyxl\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    astroid-2.5                |   py36h5fab9bb_1         297 KB  conda-forge\n",
      "    babel-2.9.0                |     pyhd3deb0d_0         6.2 MB  conda-forge\n",
      "    bleach-3.3.0               |     pyh44b312d_0         111 KB  conda-forge\n",
      "    bokeh-2.2.3                |   py36h5fab9bb_0         7.0 MB  conda-forge\n",
      "    colorama-0.4.4             |     pyh9f0ad1d_0          18 KB  conda-forge\n",
      "    dataclasses-0.7            |     pyhe4b4509_6          21 KB  conda-forge\n",
      "    docutils-0.16              |   py36h5fab9bb_3         738 KB  conda-forge\n",
      "    flask-cors-3.0.8           |             py_0          14 KB  conda-forge\n",
      "    fsspec-0.8.7               |     pyhd8ed1ab_0          72 KB  conda-forge\n",
      "    jupyterlab-3.0.9           |     pyhd8ed1ab_0         5.6 MB  conda-forge\n",
      "    lxml-4.6.2                 |   py36h04a5ba7_1         1.5 MB  conda-forge\n",
      "    nest-asyncio-1.4.3         |     pyhd8ed1ab_0           9 KB  conda-forge\n",
      "    notebook-6.2.0             |   py36h5fab9bb_0         6.2 MB  conda-forge\n",
      "    openpyxl-3.0.5             |             py_0         153 KB  anaconda\n",
      "    packaging-20.9             |     pyh44b312d_0          35 KB  conda-forge\n",
      "    pillow-8.1.0               |   py36ha6010c0_2         670 KB  conda-forge\n",
      "    pip-21.0.1                 |     pyhd8ed1ab_0         1.1 MB  conda-forge\n",
      "    pylint-2.6.0               |   py36h5fab9bb_1         452 KB  conda-forge\n",
      "    requests-2.13.0            |           py36_0         780 KB  conda-forge\n",
      "    send2trash-1.5.0           |             py_0          12 KB  conda-forge\n",
      "    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge\n",
      "    urllib3-1.26.3             |     pyhd8ed1ab_0          99 KB  conda-forge\n",
      "    werkzeug-1.0.1             |     pyh9f0ad1d_0         239 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        31.2 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  babel                          anaconda::babel-2.8.0-py_0 --> conda-forge::babel-2.9.0-pyhd3deb0d_0\n",
      "  bleach                        anaconda::bleach-3.2.1-py_0 --> conda-forge::bleach-3.3.0-pyh44b312d_0\n",
      "  dataclasses        anaconda/linux-64::dataclasses-0.7-py~ --> conda-forge/noarch::dataclasses-0.7-pyhe4b4509_6\n",
      "  docutils                   anaconda::docutils-0.16-py36_1 --> conda-forge::docutils-0.16-py36h5fab9bb_3\n",
      "  fsspec                        anaconda::fsspec-0.8.3-py_0 --> conda-forge::fsspec-0.8.7-pyhd8ed1ab_0\n",
      "  lxml                                         3.8.0-py36_0 --> 4.6.2-py36h04a5ba7_1\n",
      "  nest-asyncio            anaconda::nest-asyncio-1.4.1-py_0 --> conda-forge::nest-asyncio-1.4.3-pyhd8ed1ab_0\n",
      "  notebook                  anaconda::notebook-6.1.4-py36_0 --> conda-forge::notebook-6.2.0-py36h5fab9bb_0\n",
      "  packaging                   anaconda::packaging-20.4-py_0 --> conda-forge::packaging-20.9-pyh44b312d_0\n",
      "  pillow              anaconda::pillow-8.0.0-py36h9a89aac_0 --> conda-forge::pillow-8.1.0-py36ha6010c0_2\n",
      "  pip                  anaconda/linux-64::pip-20.2.4-py36_0 --> conda-forge/noarch::pip-21.0.1-pyhd8ed1ab_0\n",
      "  urllib3                    anaconda::urllib3-1.25.11-py_0 --> conda-forge::urllib3-1.26.3-pyhd8ed1ab_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  bokeh                        anaconda::bokeh-2.2.3-py36_0 --> conda-forge::bokeh-2.2.3-py36h5fab9bb_0\n",
      "  colorama                    anaconda::colorama-0.4.4-py_0 --> conda-forge::colorama-0.4.4-pyh9f0ad1d_0\n",
      "  flask-cors                anaconda::flask-cors-3.0.9-py_0 --> conda-forge::flask-cors-3.0.8-py_0\n",
      "  openpyxl           conda-forge::openpyxl-3.0.6-pyhd8ed1a~ --> anaconda::openpyxl-3.0.5-py_0\n",
      "  requests           conda-forge/noarch::requests-2.27.1-p~ --> conda-forge/linux-64::requests-2.13.0-py36_0\n",
      "  send2trash         anaconda/linux-64::send2trash-1.5.0-p~ --> conda-forge/noarch::send2trash-1.5.0-py_0\n",
      "  typing_extensions                                anaconda --> conda-forge\n",
      "  werkzeug                    anaconda::werkzeug-1.0.1-py_0 --> conda-forge::werkzeug-1.0.1-pyh9f0ad1d_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  astroid                              2.5.8-py36h5fab9bb_0 --> 2.5-py36h5fab9bb_1\n",
      "  jupyterlab                             3.2.8-pyhd8ed1ab_0 --> 3.0.9-pyhd8ed1ab_0\n",
      "  pylint                               2.7.2-py36h5fab9bb_0 --> 2.6.0-py36h5fab9bb_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "dataclasses-0.7      | 21 KB     | ##################################### | 100% \n",
      "flask-cors-3.0.8     | 14 KB     | ##################################### | 100% \n",
      "notebook-6.2.0       | 6.2 MB    | ##################################### | 100% \n",
      "babel-2.9.0          | 6.2 MB    | ##################################### | 100% \n",
      "bleach-3.3.0         | 111 KB    | ##################################### | 100% \n",
      "openpyxl-3.0.5       | 153 KB    | ##################################### | 100% \n",
      "urllib3-1.26.3       | 99 KB     | ##################################### | 100% \n",
      "send2trash-1.5.0     | 12 KB     | ##################################### | 100% \n",
      "colorama-0.4.4       | 18 KB     | ##################################### | 100% \n",
      "pip-21.0.1           | 1.1 MB    | ##################################### | 100% \n",
      "astroid-2.5          | 297 KB    | ##################################### | 100% \n",
      "pillow-8.1.0         | 670 KB    | ##################################### | 100% \n",
      "docutils-0.16        | 738 KB    | ##################################### | 100% \n",
      "fsspec-0.8.7         | 72 KB     | ##################################### | 100% \n",
      "jupyterlab-3.0.9     | 5.6 MB    | ##################################### | 100% \n",
      "pylint-2.6.0         | 452 KB    | ##################################### | 100% \n",
      "packaging-20.9       | 35 KB     | ##################################### | 100% \n",
      "nest-asyncio-1.4.3   | 9 KB      | ##################################### | 100% \n",
      "lxml-4.6.2           | 1.5 MB    | ##################################### | 100% \n",
      "typing_extensions-3. | 25 KB     | ##################################### | 100% \n",
      "werkzeug-1.0.1       | 239 KB    | ##################################### | 100% \n",
      "bokeh-2.2.3          | 7.0 MB    | ##################################### | 100% \n",
      "requests-2.13.0      | 780 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c anaconda openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8790414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandasql as psql\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9947d",
   "metadata": {},
   "source": [
    "# Functions to Read in Raw Data and Impute with Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ea09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_time(df, interval=\"15min\", min_date=None, max_date=None):\n",
    "    \"\"\"Function to generate full df of dates to determine which ones have missing data\"\"\"\n",
    "    \n",
    "    # Specify the start and end timestamps for the full date range\n",
    "    if not min_date:\n",
    "        start_date = df[\"timestamp\"].min()\n",
    "    else:\n",
    "        start_date = min_date\n",
    "    \n",
    "    if not max_date:\n",
    "        end_date = df[\"timestamp\"].max()\n",
    "    else:\n",
    "        end_date = max_date\n",
    "    \n",
    "    # Create a full date range using specified freq, typically either 15min or 5 min depending on the data \n",
    "    date_list = pd.date_range(start=start_date, end=end_date, freq=interval)\n",
    "    \n",
    "    # Put this date range into a df\n",
    "    date_df = pd.DataFrame({\"timestamp\": date_list})\n",
    "    \n",
    "    # Create columns for date, day of week, and day of year in addition to the timestamp column\n",
    "    date_df[\"date\"] = pd.to_datetime(date_df[\"timestamp\"].astype(\"string\").str[:10])\n",
    "    date_df[\"day_of_week\"] = date_df[\"date\"].dt.dayofweek\n",
    "    date_df[\"day_of_year\"] = date_df[\"date\"].dt.dayofyear\n",
    "    \n",
    "    return date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e07f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_portland(fname, highway_number, min_date=None, max_date=None):\n",
    "    \"\"\"Function to read in data from a full highway in Portland/Vancouver and return a df with only\n",
    "    the detector with best data availability and only necessary columns\"\"\"\n",
    "    \n",
    "    # Read in the csv with pandas\n",
    "    df = pd.read_csv(fname)\n",
    "    \n",
    "    # Add date field\n",
    "    df[\"date\"] = pd.to_datetime(df[\"starttime\"].astype(\"str\").str[:10])\n",
    "    df[\"mnth\"] = df[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "    # Group by detector and count, sort, \n",
    "    # and grab the last detector in the object (this will have the highest count)\n",
    "    best_det = df.groupby([\"detector_id\"]).count().reset_index()[[\"detector_id\", \"volume\"]]\\\n",
    "    .sort_values(by=\"volume\").iloc[-1,0]\n",
    "\n",
    "    # Subset the dataframe to only this detector and return the subset\n",
    "    df_sub = df.query(\"detector_id == @best_det\").copy()\n",
    "\n",
    "    # Set the site name in the df\n",
    "    df_sub[\"site_name\"] = f\"{highway_number}/\" + df_sub[\"detector_id\"].astype(\"str\") \n",
    "\n",
    "    # Get the time period ending in the same format as highways England    \n",
    "    df_sub[\"time_period_start\"] = df_sub[\"starttime\"].astype(\"str\").str[11:19]\n",
    "    df_sub[\"time_period_ending\"] = (pd.to_datetime(df_sub[\"time_period_start\"]) + datetime.timedelta(minutes=14)).dt.time.astype(\"str\")\n",
    "    df_sub[\"timestamp\"] = pd.to_datetime(df_sub[\"date\"].astype(\"str\") + \" \" + df_sub[\"time_period_ending\"])\n",
    "\n",
    "    # Rename columns\n",
    "    df_sub[\"total_volume\"] = df_sub[\"volume\"]\n",
    "    df_sub[\"avg_mph\"] = df_sub[\"speed\"]\n",
    "\n",
    "    # Grab only needed columns\n",
    "    df_sub = df_sub[[\"site_name\", \"timestamp\", \"avg_mph\", \"total_volume\"]]\n",
    "\n",
    "    dates = full_time(df_sub, interval=\"15min\", min_date=min_date, max_date=max_date)\n",
    "\n",
    "    # Merge full date list with actual data\n",
    "    df = dates.merge(df_sub, how=\"left\", on=\"timestamp\")\n",
    "\n",
    "    site = df[\"site_name\"].unique()[0]\n",
    "\n",
    "    df.fillna({\"site_name\": site}, inplace=True)\n",
    "\n",
    "    # Use pandasql to impute the 'interval_of_day' field \n",
    "    interval_of_day_impute = \"\"\"\n",
    "    SELECT site_name,\n",
    "           day_of_week,\n",
    "           date(date) AS date,\n",
    "           day_of_year,\n",
    "           timestamp,\n",
    "           DENSE_RANK() OVER (PARTITION BY DATE ORDER BY timestamp) - 1 AS interval_of_day,\n",
    "           avg_mph,\n",
    "           total_volume\n",
    "    FROM df\n",
    "    \"\"\"\n",
    "    df = psql.sqldf(interval_of_day_impute, locals())\n",
    "\n",
    "    # Create field with T/F if speed data is missing\n",
    "    df[\"missing_speed\"] = np.where(df[\"avg_mph\"].isnull(), True, False)\n",
    "\n",
    "    # Create field with T/F if volume data is missing\n",
    "    df[\"missing_volume\"] = np.where(df[\"total_volume\"].isnull(), True, False)\n",
    "\n",
    "    # Use linear interoplation to fill in nulls\n",
    "    df[\"avg_mph\"] = df[\"avg_mph\"].interpolate()\n",
    "    df[\"total_volume\"] = df[\"total_volume\"].interpolate()\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df, best_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ffbce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_highways_england(fname, min_date=None, max_date=None):\n",
    "    \"\"\"Function to read in csv file of highway sensor data\"\"\"\n",
    "    \n",
    "    # Read file into Pandas df\n",
    "    df = pd.read_csv(fname)\n",
    "    \n",
    "    # Grab relevant columns from df\n",
    "    df = df[[\"Site Name\", \"Report Date\", \"Time Period Ending\", \"Time Interval\", \"Avg mph\", \"Total Volume\"]]\n",
    "    \n",
    "    # Re-format date field and cast to string\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Report Date\"], format='%d/%m/%Y 00:00:00').astype(\"string\") \n",
    "    \n",
    "    # Grab the timestamp of the time-period in the hour\n",
    "    df[\"Time Period Ending\"] = df[\"Time Period Ending\"].astype(\"string\")\n",
    "    \n",
    "    # Create a true timestamp which includes both date and hour and minutes\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time Period Ending\"])\n",
    "    \n",
    "    # Subset columns and rename to include _ to make columns easier to work with\n",
    "    df = df[[\"Site Name\", \"Timestamp\", \"Time Interval\", \"Avg mph\", \"Total Volume\"]]\\\n",
    "    .rename(columns={\"Site Name\": \"site_name\",\n",
    "                     \"Timestamp\":\"timestamp\",\n",
    "                     \"Time Interval\": \"interval_of_day\",\n",
    "                     \"Avg mph\": \"avg_mph\",\n",
    "                     \"Total Volume\": \"total_volume\"})\n",
    "    \n",
    "    # Compute dates for left join \n",
    "    dates = full_time(df, interval=\"15min\", min_date=min_date, max_date=max_date)\n",
    "    \n",
    "    # Merge full date list with actual data\n",
    "    df = dates.merge(df, how=\"left\", on=\"timestamp\")\n",
    "    \n",
    "    site = df[\"site_name\"].unique()[0]\n",
    "    \n",
    "    df.fillna({\"site_name\": site}, inplace=True)\n",
    "    \n",
    "    # Use pandasql to impute the 'interval_of_day' field \n",
    "    interval_of_day_impute = \"\"\"\n",
    "    SELECT site_name,\n",
    "           day_of_week,\n",
    "           date(date) AS date,\n",
    "           day_of_year,\n",
    "           timestamp,\n",
    "           DENSE_RANK() OVER (PARTITION BY DATE ORDER BY timestamp) - 1 AS interval_of_day,\n",
    "           avg_mph,\n",
    "           total_volume\n",
    "    FROM df\n",
    "    \"\"\"\n",
    "    df = psql.sqldf(interval_of_day_impute, locals())\n",
    "    \n",
    "    # Create field with T/F if speed data is missing\n",
    "    df[\"missing_speed\"] = np.where(df[\"avg_mph\"].isnull(), True, False)\n",
    "    \n",
    "    # Create field with T/F if volume data is missing\n",
    "    df[\"missing_volume\"] = np.where(df[\"total_volume\"].isnull(), True, False)\n",
    "\n",
    "    # Set DateTime Index\n",
    "#     df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "#     df = df.set_index(\"timestamp\")\n",
    "    \n",
    "    # Use linear interoplation to fill in nulls\n",
    "    df[\"avg_mph\"] = df[\"avg_mph\"].interpolate()\n",
    "    df[\"total_volume\"] = df[\"total_volume\"].interpolate()\n",
    "    \n",
    "#     df = df.reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff26abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_utah(folder_path, highway_number, detector_id, min_date=None, max_date=None):\n",
    "    \"\"\"Function to read in data from a UTAH PEMS sensor and aggregate from 5min to 15min to be the same as \n",
    "    other data sources in our set (Highways England, Portland-Vancouver Regional)\"\"\"\n",
    "    \n",
    "    # Grab all excel files in the provided path, there are typically 13-14 files per sensor and year\n",
    "    flist = glob.glob(f\"{folder_path}/*.xlsx\")\n",
    "    df = pd.DataFrame()\n",
    "    for fname in flist:\n",
    "        df_sub = pd.read_excel(fname, engine=\"openpyxl\")\n",
    "        df = df.append(df_sub)\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    df[\"date\"] = df[\"5 Minutes\"].astype(\"str\").str[0:11]\n",
    "    df[\"time_period_start\"] = df[\"5 Minutes\"].astype(\"str\").str[11:19]\n",
    "    df[\"time_period_ending\"] = (pd.to_datetime(df[\"time_period_start\"]) + datetime.timedelta(minutes=4)).dt.time.astype(\"str\")\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"date\"] + \" \" + df[\"time_period_ending\"]).round(\"min\")\n",
    "    df[\"total_volume\"] = df[\"Flow (Veh/5 Minutes)\"]\n",
    "    df[\"avg_mph\"] = df[\"Speed (mph)\"]\n",
    "    df[\"site_name\"] = f\"{highway_number}/{detector_id}\"\n",
    "    \n",
    "    df = df[[\"site_name\", \"timestamp\", \"avg_mph\", \"total_volume\", \"time_period_start\"]].copy()\n",
    "    \n",
    "    df = df.query(\"timestamp >= '2019-01-01 00:00:00'\").query(\"timestamp < '2020-01-01 00:00:00'\").copy()\n",
    "    \n",
    "    date_df = full_time(df, interval=\"5min\", min_date=min_date, max_date=max_date)\n",
    "    \n",
    "    full_df = date_df.merge(df, how=\"left\", on=\"timestamp\")\n",
    "    \n",
    "    site_name = full_df.site_name.unique()[0]\n",
    "    full_df.fillna({\"site_name\": site_name}, inplace=True)\n",
    "    \n",
    "    interval_of_day = \"\"\"\n",
    "    SELECT  site_name,\n",
    "            day_of_week,\n",
    "            date(date) AS date,\n",
    "            day_of_year,\n",
    "            timestamp,\n",
    "            DENSE_RANK() OVER (PARTITION BY DATE ORDER BY timestamp) - 1 AS interval_of_day,\n",
    "            avg_mph,\n",
    "            total_volume \n",
    "    FROM full_df\n",
    "    \"\"\"\n",
    "    full_df = psql.sqldf(interval_of_day, locals())\n",
    "    \n",
    "    # Create field with T/F if speed data is missing\n",
    "    full_df[\"missing_speed\"] = np.where(full_df[\"avg_mph\"].isnull(), True, False)\n",
    "    \n",
    "    # Create field with T/F if volume data is missing\n",
    "    full_df[\"missing_volume\"] = np.where(full_df[\"total_volume\"].isnull(), True, False)\n",
    "    \n",
    "    # Use linear interoplation to fill in nulls\n",
    "    df[\"avg_mph\"] = df[\"avg_mph\"].interpolate()\n",
    "    df[\"total_volume\"] = df[\"total_volume\"].interpolate()\n",
    "        \n",
    "    # Aggregate up to 15 minute intervals - this section is moved to R because of linear interp not working well\n",
    "#     full_df[\"min_15_int\"] = full_df[\"interval_of_day\"] // 3\n",
    "    \n",
    "#     full_df = full_df.groupby([\"site_name\", \"day_of_week\", \"date\", \"day_of_year\",\"min_15_int\"]).\\\n",
    "#     agg({\"timestamp\": \"max\", \"total_volume\": \"sum\", \"avg_mph\": \"mean\", \n",
    "#          \"missing_speed\": \"max\", \"missing_volume\": \"max\"}).reset_index()\n",
    "    \n",
    "#     full_df.rename(columns={\"min_15_int\": \"interval_of_day\"}, inplace=True)\n",
    "    \n",
    "    full_df = full_df[[\"site_name\",\n",
    "                       \"day_of_week\",\n",
    "                       \"date\",\n",
    "                       \"day_of_year\",\n",
    "                       \"timestamp\",\n",
    "                       \"interval_of_day\",\n",
    "                       \"avg_mph\",\n",
    "                       \"total_volume\",\n",
    "                       \"missing_speed\",\n",
    "                       \"missing_volume\"]].copy()\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40383725",
   "metadata": {},
   "source": [
    "# Read in Raw Data, Impute, and Write to New Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eec2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date_5 = \"2019-01-01 00:04:00\"\n",
    "min_date_15 = \"2019-01-01 00:14:00\"\n",
    "max_date = \"2019-12-31 23:59:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85c1ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370ed7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data/Raw/Highways_England/M18-7578A_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M25-4490B_Counterclockwise_2019.csv\n",
      "Reading Data/Raw/Highways_England/M6-5441A_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M11-6747A_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A14-1107A_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A64-9251-1_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A11-6312-2_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M5-8291A_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M20-6552A2_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M60-9327B_Counterclockwise_2019.csv\n",
      "Reading Data/Raw/Highways_England/M60-9374A_Clockwise_2019.csv\n",
      "Reading Data/Raw/Highways_England/M25-4565A_Clockwise_2019.csv\n",
      "Reading Data/Raw/Highways_England/A5-7572-1-Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M1-2633A_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A1M-9842B_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M20-6572B2_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A69-9785-1_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M6-7036B_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M11-6400B_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A47-6337-2_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A14-1144B_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A11-6310-1_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M3-1537L_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M4-3434B_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A590-9634-1_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A5-6847-2_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A64-9252-1_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A590-9531-1_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M3-1524A_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M18-7569B_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A69-9784-1_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M4-2156A_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A47-6337-1_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A46-7636-2_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A46-7636-1_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A1M-9847a_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M1-2148L_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M5-7650B_Northbound_2019.csv\n"
     ]
    }
   ],
   "source": [
    "for fname in glob.glob(\"Data/Raw/Highways_England/*.csv\"):\n",
    "    print(\"Reading {}\".format(fname))\n",
    "    \n",
    "    fname_new = \"Data/Intermediate/Highways_England/{}_Intermediate.csv\".format(fname.split(\"/\")[-1].split(\".\")[0])\n",
    "    \n",
    "    df = read_highways_england(fname, min_date_15, max_date)\n",
    "    \n",
    "    df.to_csv(fname_new, index=False)\n",
    "    \n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c55b5ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data/Raw/Portland/all_sensors_highway51_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway7_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway52_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway4_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway12_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway1_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway6_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway10_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway2_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway621_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway50_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway9_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway53_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway620_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway11_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway618_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway8_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway5_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway3_portland.csv\n"
     ]
    }
   ],
   "source": [
    "portland_highway_numbers = pd.read_csv(\"Data/Metadata/portland_highway_metadata.csv\")\n",
    "portland_highway_numbers['highwayid'] = portland_highway_numbers['highwayid'].astype(\"str\")\n",
    "\n",
    "for fname in glob.glob(\"Data/Raw/Portland/*.csv\"):\n",
    "    print(\"Reading {}\".format(fname))\n",
    "    \n",
    "    highway_id = fname.split(\"highway\")[-1].split(\"_\")[0]\n",
    "    highway_name = portland_highway_numbers.query(\"highwayid==@highway_id\")[\"highwayname\"].iloc[0].replace(\"-\", \"\")\n",
    "    direction = portland_highway_numbers.query(\"highwayid==@highway_id\")[\"direction\"].iloc[0].capitalize()\n",
    "    \n",
    "    df, det = read_portland(fname, highway_name, min_date=min_date_15, max_date=max_date)\n",
    "    \n",
    "    fname_new = \"Data/Intermediate/Portland/{}-{}_{}bound_2019_Intermediate.csv\".format(highway_name, det, direction)\n",
    "    \n",
    "    df.to_csv(fname_new, index=False)\n",
    "    \n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7de161ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files in Data/Raw/Utah/US189_260_Westbound directory\n",
      "Reading files in Data/Raw/Utah/I15_3103178_Southbound directory\n",
      "Reading files in Data/Raw/Utah/I215_134_Counterclockwise directory\n",
      "Reading files in Data/Raw/Utah/I80_667_Westbound directory\n",
      "Reading files in Data/Raw/Utah/US189_470_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/LegacyParkway_810_Northbound directory\n",
      "Reading files in Data/Raw/Utah/US6_3103115_Westbound directory\n",
      "Reading files in Data/Raw/Utah/US40_635_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/I84_482_Westbound directory\n",
      "Reading files in Data/Raw/Utah/I70_3103401_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/I70_3103400_Westbound directory\n",
      "Reading files in Data/Raw/Utah/US89_483_Northbound directory\n",
      "Reading files in Data/Raw/Utah/I80_600_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/I215_31_Clockwise directory\n",
      "Reading files in Data/Raw/Utah/US40_634_Westbound directory\n",
      "Reading files in Data/Raw/Utah/I84_451_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/LegacyParkway_890_Southbound directory\n",
      "Reading files in Data/Raw/Utah/US6_3103114_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/I15_749_Northbound directory\n"
     ]
    }
   ],
   "source": [
    "for path in glob.glob(\"Data/Raw/Utah/*\"):\n",
    "    print(\"Reading files in {} directory\".format(path))\n",
    "    \n",
    "    highway_number = path.split(\"/\")[-1].split(\"_\")[0]\n",
    "    detector_id = path.split(\"/\")[-1].split(\"_\")[1]\n",
    "    direction = path.split(\"/\")[-1].split(\"_\")[2]\n",
    "    \n",
    "    df = read_utah(path, highway_number, detector_id, min_date=min_date_5, max_date=max_date)\n",
    "    \n",
    "    fname_new = \"Data/Intermediate/Utah/{}-{}_{}_2019_Intermediate.csv\".format(highway_number, detector_id, direction)\n",
    "    \n",
    "    df.to_csv(fname_new, index=False)\n",
    "    \n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51c91ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e598d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
