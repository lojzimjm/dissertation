{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f76ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the libraries are not yet installed, they can be installed using conda commands similar to the below\n",
    "# %conda install numpy\n",
    "# %conda install pandas\n",
    "# %conda install pandasql \n",
    "# %conda install openpyxl\n",
    "\n",
    "# Something like the following may also work if the above does not\n",
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} numpy\n",
    "# !conda install --yes --prefix {sys.prefix} pandas\n",
    "# !conda install --yes --prefix {sys.prefix} pandasql\n",
    "# !conda install --yes --prefix {sys.prefix} openpyxl\n",
    "\n",
    "# To install a specific version, add the version to the install command\n",
    "# E.g., %conda install numpy=1.20.3\n",
    "\n",
    "# If all else fails, use pip or follow additional advice such as found at\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "\n",
    "# If your plan to use pip (especially if you are not working within a specified conda environment), \n",
    "# the pip commands might look like:\n",
    "# pip install numpy\n",
    "# pip install pandas\n",
    "# pip install pandasql\n",
    "# pip install open pyxl\n",
    "\n",
    "# To install a specific version, add the version to the pip install command\n",
    "# E.g., pip install numpy==1.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e89045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandasql as psql\n",
    "import glob\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eecf208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create intermediate and processed directories for later use. We use the makedirs method because this can \n",
    "# create the directories recursively, and we set exist_ok to True to handle the case where the directories\n",
    "# already exist\n",
    "os.makedirs(\"Data/Intermediate/Highways_England\", exist_ok=True)\n",
    "os.makedirs(\"Data/Intermediate/Utah\", exist_ok=True)\n",
    "os.makedirs(\"Data/Intermediate/Portland\", exist_ok=True)\n",
    "\n",
    "os.makedirs(\"Data/Processed/Highways_England\", exist_ok=True)\n",
    "os.makedirs(\"Data/Processed/Utah\", exist_ok=True)\n",
    "os.makedirs(\"Data/Processed/Portland\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33ca94",
   "metadata": {},
   "source": [
    "# Functions to Read in Raw Data and Impute with Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9ecf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_time(df, interval=\"15min\", min_date=None, max_date=None):\n",
    "    \"\"\"Function to generate full df of dates to determine which ones have missing data\"\"\"\n",
    "    \n",
    "    # Specify the start and end timestamps for the full date range\n",
    "    if not min_date:\n",
    "        start_date = df[\"timestamp\"].min()\n",
    "    else:\n",
    "        start_date = min_date\n",
    "    \n",
    "    if not max_date:\n",
    "        end_date = df[\"timestamp\"].max()\n",
    "    else:\n",
    "        end_date = max_date\n",
    "    \n",
    "    # Create a full date range using specified freq, typically either 15min or 5 min depending on the data \n",
    "    date_list = pd.date_range(start=start_date, end=end_date, freq=interval)\n",
    "    \n",
    "    # Put this date range into a df\n",
    "    date_df = pd.DataFrame({\"timestamp\": date_list})\n",
    "    \n",
    "    # Create columns for date, day of week, and day of year in addition to the timestamp column\n",
    "    date_df[\"date\"] = pd.to_datetime(date_df[\"timestamp\"].astype(\"string\").str[:10])\n",
    "    date_df[\"day_of_week\"] = date_df[\"date\"].dt.dayofweek\n",
    "    date_df[\"day_of_year\"] = date_df[\"date\"].dt.dayofyear\n",
    "    \n",
    "    return date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "110a0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_portland(fname, highway_number, highway_id, min_date=None, max_date=None):\n",
    "    \"\"\"Function to read in data from a full highway in Portland/Vancouver and return a df with only\n",
    "    the detector with best data availability and only necessary columns\"\"\"\n",
    "    \n",
    "    # Read in the csv with pandas\n",
    "    df = pd.read_csv(fname)\n",
    "    \n",
    "    # Add date field\n",
    "    df[\"date\"] = pd.to_datetime(df[\"starttime\"].astype(\"str\").str[:10])\n",
    "    df[\"mnth\"] = df[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "    if highway_id == '50': \n",
    "        # Highway 50 has several good detectors, so we specify that we want 102003\n",
    "        best_det = 102003\n",
    "    elif highway_id == '10':\n",
    "        # For each of the elif statements, we are specifying the specific detector \n",
    "        # because these highways have more than one \"best detector\" in terms of data availability\n",
    "        best_det = 100300\n",
    "    elif highway_id == '12':\n",
    "        best_det = 100650\n",
    "    elif highway_id == '1':\n",
    "        best_det = 100703\n",
    "    elif highway_id == '2':\n",
    "        best_det = 100688\n",
    "    elif highway_id == '51':\n",
    "        best_det = 102001\n",
    "    elif highway_id == '52':\n",
    "        best_det = 1000022\n",
    "    elif highway_id == '53':\n",
    "        best_det = 1000104\n",
    "    elif highway_id == '618':\n",
    "        best_det = 101745\n",
    "    elif highway_id == '6':\n",
    "        best_det = 100395\n",
    "    elif highway_id == '7':\n",
    "        best_det = 101108\n",
    "    elif highway_id == '8':\n",
    "        best_det = 101161\n",
    "    elif highway_id == '9':\n",
    "        best_det = 100314\n",
    "    else:  \n",
    "        # If we have not specified a detector, we group by detector and count, sort, \n",
    "        # and grab the last detector in the object (this will have the highest count)\n",
    "        best_det = df.groupby([\"detector_id\"]).count().reset_index()[[\"detector_id\", \"volume\"]]\\\n",
    "        .sort_values(by=\"volume\").iloc[-1,0]\n",
    "\n",
    "    # Subset the dataframe to only this detector and return the subset\n",
    "    df_sub = df.query(\"detector_id == @best_det\").copy()\n",
    "\n",
    "    # Set the site name in the df\n",
    "    df_sub[\"site_name\"] = f\"{highway_number}/\" + df_sub[\"detector_id\"].astype(\"str\") \n",
    "\n",
    "    # Get the time period ending in the same format as highways England    \n",
    "    df_sub[\"time_period_start\"] = df_sub[\"starttime\"].astype(\"str\").str[11:19]\n",
    "    df_sub[\"time_period_ending\"] = (pd.to_datetime(df_sub[\"time_period_start\"]) + datetime.timedelta(minutes=14)).dt.time.astype(\"str\")\n",
    "    df_sub[\"timestamp\"] = pd.to_datetime(df_sub[\"date\"].astype(\"str\") + \" \" + df_sub[\"time_period_ending\"])\n",
    "\n",
    "    # Rename columns\n",
    "    df_sub[\"total_volume\"] = df_sub[\"volume\"]\n",
    "    df_sub[\"avg_mph\"] = df_sub[\"speed\"]\n",
    "\n",
    "    # Grab only needed columns\n",
    "    df_sub = df_sub[[\"site_name\", \"timestamp\", \"avg_mph\", \"total_volume\"]]\n",
    "\n",
    "    dates = full_time(df_sub, interval=\"15min\", min_date=min_date, max_date=max_date)\n",
    "\n",
    "    # Merge full date list with actual data\n",
    "    df = dates.merge(df_sub, how=\"left\", on=\"timestamp\")\n",
    "\n",
    "    site = df[\"site_name\"].unique()[0]\n",
    "\n",
    "    df.fillna({\"site_name\": site}, inplace=True)\n",
    "\n",
    "    # Use pandasql to impute the 'interval_of_day' field \n",
    "    interval_of_day_impute = \"\"\"\n",
    "    SELECT site_name,\n",
    "           day_of_week,\n",
    "           date(date) AS date,\n",
    "           day_of_year,\n",
    "           timestamp,\n",
    "           DENSE_RANK() OVER (PARTITION BY DATE ORDER BY timestamp) - 1 AS interval_of_day,\n",
    "           avg_mph,\n",
    "           total_volume\n",
    "    FROM df\n",
    "    \"\"\"\n",
    "    df = psql.sqldf(interval_of_day_impute, locals())\n",
    "\n",
    "    # Create field with T/F if speed data is missing\n",
    "    df[\"missing_speed\"] = np.where(df[\"avg_mph\"].isnull(), True, False)\n",
    "\n",
    "    # Create field with T/F if volume data is missing\n",
    "    df[\"missing_volume\"] = np.where(df[\"total_volume\"].isnull(), True, False)\n",
    "\n",
    "    # Use linear interoplation to fill in nulls\n",
    "    df[\"avg_mph\"] = df[\"avg_mph\"].interpolate()\n",
    "    df[\"total_volume\"] = df[\"total_volume\"].interpolate()\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df, best_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a583ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_highways_england(fname, min_date=None, max_date=None):\n",
    "    \"\"\"Function to read in csv file of highway sensor data\"\"\"\n",
    "    \n",
    "    # Read file into Pandas df\n",
    "    df = pd.read_csv(fname)\n",
    "    \n",
    "    # Grab relevant columns from df\n",
    "    df = df[[\"Site Name\", \"Report Date\", \"Time Period Ending\", \"Time Interval\", \"Avg mph\", \"Total Volume\"]]\n",
    "    \n",
    "    # Re-format date field and cast to string\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Report Date\"], format='%d/%m/%Y 00:00:00').astype(\"string\") \n",
    "    \n",
    "    # Grab the timestamp of the time-period in the hour\n",
    "    df[\"Time Period Ending\"] = df[\"Time Period Ending\"].astype(\"string\")\n",
    "    \n",
    "    # Create a true timestamp which includes both date and hour and minutes\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time Period Ending\"])\n",
    "    \n",
    "    # Subset columns and rename to include _ to make columns easier to work with\n",
    "    df = df[[\"Site Name\", \"Timestamp\", \"Time Interval\", \"Avg mph\", \"Total Volume\"]]\\\n",
    "    .rename(columns={\"Site Name\": \"site_name\",\n",
    "                     \"Timestamp\":\"timestamp\",\n",
    "                     \"Time Interval\": \"interval_of_day\",\n",
    "                     \"Avg mph\": \"avg_mph\",\n",
    "                     \"Total Volume\": \"total_volume\"})\n",
    "    \n",
    "    # Compute dates for left join \n",
    "    dates = full_time(df, interval=\"15min\", min_date=min_date, max_date=max_date)\n",
    "    \n",
    "    # Merge full date list with actual data\n",
    "    df = dates.merge(df, how=\"left\", on=\"timestamp\")\n",
    "    \n",
    "    site = df[\"site_name\"].unique()[0]\n",
    "    \n",
    "    df.fillna({\"site_name\": site}, inplace=True)\n",
    "    \n",
    "    # Use pandasql to impute the 'interval_of_day' field \n",
    "    interval_of_day_impute = \"\"\"\n",
    "    SELECT site_name,\n",
    "           day_of_week,\n",
    "           date(date) AS date,\n",
    "           day_of_year,\n",
    "           timestamp,\n",
    "           DENSE_RANK() OVER (PARTITION BY DATE ORDER BY timestamp) - 1 AS interval_of_day,\n",
    "           avg_mph,\n",
    "           total_volume\n",
    "    FROM df\n",
    "    \"\"\"\n",
    "    df = psql.sqldf(interval_of_day_impute, locals())\n",
    "    \n",
    "    # Create field with T/F if speed data is missing\n",
    "    df[\"missing_speed\"] = np.where(df[\"avg_mph\"].isnull(), True, False)\n",
    "    \n",
    "    # Create field with T/F if volume data is missing\n",
    "    df[\"missing_volume\"] = np.where(df[\"total_volume\"].isnull(), True, False)\n",
    "\n",
    "    # Set DateTime Index\n",
    "#     df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "#     df = df.set_index(\"timestamp\")\n",
    "    \n",
    "    # Use linear interoplation to fill in nulls\n",
    "    df[\"avg_mph\"] = df[\"avg_mph\"].interpolate()\n",
    "    df[\"total_volume\"] = df[\"total_volume\"].interpolate()\n",
    "    \n",
    "#     df = df.reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30d44a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_utah(folder_path, highway_number, detector_id, min_date=None, max_date=None):\n",
    "    \"\"\"Function to read in data from a UTAH PEMS sensor and aggregate from 5min to 15min to be the same as \n",
    "    other data sources in our set (Highways England, Portland-Vancouver Regional)\"\"\"\n",
    "    \n",
    "    # Grab all excel files in the provided path, there are typically 13-14 files per sensor and year\n",
    "    flist = glob.glob(f\"{folder_path}/*.xlsx\")\n",
    "    df = pd.DataFrame()\n",
    "    for fname in flist:\n",
    "        df_sub = pd.read_excel(fname, engine=\"openpyxl\")\n",
    "        df = df.append(df_sub)\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    df[\"date\"] = df[\"5 Minutes\"].astype(\"str\").str[0:11]\n",
    "    df[\"time_period_start\"] = df[\"5 Minutes\"].astype(\"str\").str[11:19]\n",
    "    df[\"time_period_ending\"] = (pd.to_datetime(df[\"time_period_start\"]) + datetime.timedelta(minutes=4)).dt.time.astype(\"str\")\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"date\"] + \" \" + df[\"time_period_ending\"]).round(\"min\")\n",
    "    df[\"total_volume\"] = df[\"Flow (Veh/5 Minutes)\"]\n",
    "    df[\"avg_mph\"] = df[\"Speed (mph)\"]\n",
    "    df[\"site_name\"] = f\"{highway_number}/{detector_id}\"\n",
    "    \n",
    "    df = df[[\"site_name\", \"timestamp\", \"avg_mph\", \"total_volume\", \"time_period_start\"]].copy()\n",
    "    \n",
    "    df = df.query(\"timestamp >= '2019-01-01 00:00:00'\").query(\"timestamp < '2020-01-01 00:00:00'\").copy()\n",
    "    \n",
    "    date_df = full_time(df, interval=\"5min\", min_date=min_date, max_date=max_date)\n",
    "    \n",
    "    full_df = date_df.merge(df, how=\"left\", on=\"timestamp\")\n",
    "    \n",
    "    site_name = full_df.site_name.unique()[0]\n",
    "    full_df.fillna({\"site_name\": site_name}, inplace=True)\n",
    "    \n",
    "    interval_of_day = \"\"\"\n",
    "    SELECT  site_name,\n",
    "            day_of_week,\n",
    "            date(date) AS date,\n",
    "            day_of_year,\n",
    "            timestamp,\n",
    "            DENSE_RANK() OVER (PARTITION BY DATE ORDER BY timestamp) - 1 AS interval_of_day,\n",
    "            avg_mph,\n",
    "            total_volume \n",
    "    FROM full_df\n",
    "    \"\"\"\n",
    "    full_df = psql.sqldf(interval_of_day, locals())\n",
    "    \n",
    "    # Create field with T/F if speed data is missing\n",
    "    full_df[\"missing_speed\"] = np.where(full_df[\"avg_mph\"].isnull(), True, False)\n",
    "    \n",
    "    # Create field with T/F if volume data is missing\n",
    "    full_df[\"missing_volume\"] = np.where(full_df[\"total_volume\"].isnull(), True, False)\n",
    "    \n",
    "    # Use linear interoplation to fill in nulls\n",
    "    df[\"avg_mph\"] = df[\"avg_mph\"].interpolate()\n",
    "    df[\"total_volume\"] = df[\"total_volume\"].interpolate()\n",
    "        \n",
    "    # Aggregate up to 15 minute intervals - this section is moved to R because of linear interp not working well\n",
    "#     full_df[\"min_15_int\"] = full_df[\"interval_of_day\"] // 3\n",
    "    \n",
    "#     full_df = full_df.groupby([\"site_name\", \"day_of_week\", \"date\", \"day_of_year\",\"min_15_int\"]).\\\n",
    "#     agg({\"timestamp\": \"max\", \"total_volume\": \"sum\", \"avg_mph\": \"mean\", \n",
    "#          \"missing_speed\": \"max\", \"missing_volume\": \"max\"}).reset_index()\n",
    "    \n",
    "#     full_df.rename(columns={\"min_15_int\": \"interval_of_day\"}, inplace=True)\n",
    "    \n",
    "    full_df = full_df[[\"site_name\",\n",
    "                       \"day_of_week\",\n",
    "                       \"date\",\n",
    "                       \"day_of_year\",\n",
    "                       \"timestamp\",\n",
    "                       \"interval_of_day\",\n",
    "                       \"avg_mph\",\n",
    "                       \"total_volume\",\n",
    "                       \"missing_speed\",\n",
    "                       \"missing_volume\"]].copy()\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242aca3d",
   "metadata": {},
   "source": [
    "# Read in Raw Data, Impute, and Write to New Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbb6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the min and max dates for the 5-min and 15-min data\n",
    "min_date_5 = \"2019-01-01 00:04:00\"\n",
    "min_date_15 = \"2019-01-01 00:14:00\"\n",
    "max_date = \"2019-12-31 23:59:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02288bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99bfe12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data/Raw/Highways_England/A11-6310-1_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A11-6312-2_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A14-1107A_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A14-1144B_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A1M-9842B_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A1M-9847a_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A46-7636-1_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A46-7636-2_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A47-6337-1_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A47-6337-2_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A5-6847-2_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A5-7572-1-Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A590-9531-1_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A590-9634-1_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A64-9251-1_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A64-9252-1_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A69-9784-1_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/A69-9785-1_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M1-2148L_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M1-2633A_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M11-6400B_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M11-6747A_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M18-7569B_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M18-7578A_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M20-6552A2_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M20-6572B2_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M25-4490B_Counterclockwise_2019.csv\n",
      "Reading Data/Raw/Highways_England/M25-4565A_Clockwise_2019.csv\n",
      "Reading Data/Raw/Highways_England/M3-1524A_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M3-1537L_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M4-2156A_Westbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M4-3434B_Eastbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M5-7650B_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M5-8291A_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M6-5441A_Northbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M6-7036B_Southbound_2019.csv\n",
      "Reading Data/Raw/Highways_England/M60-9327B_Counterclockwise_2019.csv\n",
      "Reading Data/Raw/Highways_England/M60-9374A_Clockwise_2019.csv\n"
     ]
    }
   ],
   "source": [
    "# Read in all raw highways england data files, impute, and write to the intermediate direcotry of the data folder\n",
    "for fname in glob.glob(\"Data/Raw/Highways_England/*.csv\"):\n",
    "    print(\"Reading {}\".format(fname))\n",
    "    \n",
    "    fname_new = \"Data/Intermediate/Highways_England/{}_Intermediate.csv\".format(fname.split(\"/\")[-1].split(\".\")[0])\n",
    "    \n",
    "    df = read_highways_england(fname, min_date_15, max_date)\n",
    "    \n",
    "    df.to_csv(fname_new, index=False)\n",
    "    \n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3c00838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data/Raw/Portland/all_sensors_highway10_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway11_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway12_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway1_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway2_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway3_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway4_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway50_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway51_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway52_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway53_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway5_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway618_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway620_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway621_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway6_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway7_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway8_portland.csv\n",
      "Reading Data/Raw/Portland/all_sensors_highway9_portland.csv\n"
     ]
    }
   ],
   "source": [
    "# Read in the highway id metadata for the portland regional highway system\n",
    "portland_highway_numbers = pd.read_csv(\"Data/Metadata/portland_highway_metadata.csv\")\n",
    "portland_highway_numbers['highwayid'] = portland_highway_numbers['highwayid'].astype(\"str\")\n",
    "\n",
    "# Read in all raw portland data files, impute, and write to the intermediate direcotry of the data folder\n",
    "for fname in glob.glob(\"Data/Raw/Portland/*.csv\"):\n",
    "    print(\"Reading {}\".format(fname))\n",
    "    \n",
    "    # Parse the highway name and id in order to properly name the new file\n",
    "    highway_id = fname.split(\"highway\")[-1].split(\"_\")[0]\n",
    "    highway_name = portland_highway_numbers.query(\"highwayid==@highway_id\")[\"highwayname\"].iloc[0].replace(\"-\", \"\")\n",
    "    direction = portland_highway_numbers.query(\"highwayid==@highway_id\")[\"direction\"].iloc[0].capitalize()\n",
    "    \n",
    "    # Read in each raw file\n",
    "    df, det = read_portland(fname, highway_name, highway_id, min_date=min_date_15, max_date=max_date)\n",
    "    \n",
    "    fname_new = \"Data/Intermediate/Portland/{}-{}_{}bound_2019_Intermediate.csv\".format(highway_name, det, direction)\n",
    "    \n",
    "    # Write the processed file\n",
    "    df.to_csv(fname_new, index=False)\n",
    "    \n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32053dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files in Data/Raw/Utah/I15_3103178_Southbound directory\n",
      "Reading files in Data/Raw/Utah/I15_749_Northbound directory\n",
      "Reading files in Data/Raw/Utah/I215_134_Counterclockwise directory\n",
      "Reading files in Data/Raw/Utah/I215_31_Clockwise directory\n",
      "Reading files in Data/Raw/Utah/I70_3103400_Westbound directory\n",
      "Reading files in Data/Raw/Utah/I70_3103401_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/I80_600_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/I80_667_Westbound directory\n",
      "Reading files in Data/Raw/Utah/I84_451_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/I84_482_Westbound directory\n",
      "Reading files in Data/Raw/Utah/LegacyParkway_810_Northbound directory\n",
      "Reading files in Data/Raw/Utah/LegacyParkway_890_Southbound directory\n",
      "Reading files in Data/Raw/Utah/US189_260_Westbound directory\n",
      "Reading files in Data/Raw/Utah/US189_470_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/US40_634_Westbound directory\n",
      "Reading files in Data/Raw/Utah/US40_635_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/US6_3103114_Eastbound directory\n",
      "Reading files in Data/Raw/Utah/US6_3103115_Westbound directory\n",
      "Reading files in Data/Raw/Utah/US89_483_Northbound directory\n"
     ]
    }
   ],
   "source": [
    "# Read in all raw utah data files, impute, and write to the intermediate directory of the data folder\n",
    "for path in glob.glob(\"Data/Raw/Utah/*\"):\n",
    "    print(\"Reading files in {} directory\".format(path))\n",
    "    \n",
    "    highway_number = path.split(\"/\")[-1].split(\"_\")[0]\n",
    "    detector_id = path.split(\"/\")[-1].split(\"_\")[1]\n",
    "    direction = path.split(\"/\")[-1].split(\"_\")[2]\n",
    "    \n",
    "    df = read_utah(path, highway_number, detector_id, min_date=min_date_5, max_date=max_date)\n",
    "    \n",
    "    fname_new = \"Data/Intermediate/Utah/{}-{}_{}_2019_Intermediate.csv\".format(highway_number, detector_id, direction)\n",
    "    \n",
    "    df.to_csv(fname_new, index=False)\n",
    "    \n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7fdd85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check the counter\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78404fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
