{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the libraries are not yet installed, they can be installed in this notebook using commands similar to the below\n",
    "# %conda install numpy\n",
    "# %conda install pandas\n",
    "# %conda install matplotlib\n",
    "# %conda install scikit-learn\n",
    "# %conda install -c conda-forge lightgbm \n",
    "# %conda install -c conda-forge swifter\n",
    "# %conda install -c conda-forge scipy\n",
    "# %conda install joblib\n",
    "# %conda install tdqm\n",
    "\n",
    "# Something like the following may also work if the above does not\n",
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} numpy\n",
    "# !conda install --yes --prefix {sys.prefix} pandas\n",
    "# !conda install --yes --prefix {sys.prefix} scikit-learn\n",
    "# !conda install -c conda-forge --yes --prefix {sys.prefix} lightgbm\n",
    "# !conda install -c conda-forge --yes --prefix {sys.prefix} swifter\n",
    "# !conda install -c conda-forge --yes --prefix {sys.prefix} scipy \n",
    "# !conda install --yes --prefix {sys.prefix} joblib\n",
    "# !conda install --yes --prefix {sys.prefix} tdqm\n",
    "\n",
    "# To install a specific version, add the version to the install command\n",
    "# E.g., %conda install numpy=1.20.3\n",
    "\n",
    "# If all else fails, use pip or follow additional advice such as found at\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "\n",
    "# If your plan to use pip (especially if you are not working within a specified conda environment), \n",
    "# the pip commands might look like:\n",
    "# pip install numpy\n",
    "# pip install pandas\n",
    "# pip install scikit-learn\n",
    "# pip install lightgbm\n",
    "# pip install swifter\n",
    "# pip install scipy\n",
    "# pip install joblib\n",
    "# pip install tdqm\n",
    "\n",
    "# To install a specific version, add the version to the pip install command\n",
    "# E.g., pip install numpy==1.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import glob\n",
    "from lightgbm import LGBMRegressor\n",
    "import random\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import scipy\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "import contextlib\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(54321)\n",
    "random.seed(54321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories to save results\n",
    "\n",
    "if not os.path.exists(\"Results/Global/LightGBM Default/\"):\n",
    "    os.mkdir(\"Results/Global/LightGBM Default\")\n",
    "      \n",
    "if not os.path.exists(\"Results/Global/LightGBM Default/Full\"):\n",
    "    os.mkdir(\"Results/Global/LightGBM Default/Full\")\n",
    "    \n",
    "if not os.path.exists(\"Results/Global/LightGBM Default/Random Cluster\"):\n",
    "    os.mkdir(\"Results/Global/LightGBM Default/Random Cluster\")\n",
    "    \n",
    "if not os.path.exists(\"Results/Global/LightGBM Default/Highway System\"):\n",
    "    os.mkdir(\"Results/Global/LightGBM Default/Highway System\")\n",
    "    \n",
    "if not os.path.exists(\"Results/Global/LightGBM Default/Catch22 KMeans\"):\n",
    "    os.mkdir(\"Results/Global/LightGBM Default/Catch22 KMeans\")\n",
    "\n",
    "if not os.path.exists(\"Results/Global/LightGBM Default/TSFeat KMeans\"):\n",
    "    os.mkdir(\"Results/Global/LightGBM Default/TSFeat KMeans\")\n",
    "\n",
    "if not os.path.exists(\"Results/Global/LightGBM Default/DTW\"):\n",
    "    os.mkdir(\"Results/Global/LightGBM Default/DTW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data and Prepare for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data/Processed/Highways_England/A11-6310-1_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A11-6312-2_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A14-1107A_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A14-1144B_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A1M-9842B_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A1M-9847a_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A46-7636-1_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A46-7636-2_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A47-6337-1_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A47-6337-2_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A5-6847-2_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A5-7572-1-Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A590-9531-1_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A590-9634-1_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A64-9251-1_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A64-9252-1_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A69-9784-1_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/A69-9785-1_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M1-2148L_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M1-2633A_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M11-6400B_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M11-6747A_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M18-7569B_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M18-7578A_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M20-6552A2_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M20-6572B2_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M25-4490B_Counterclockwise_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M25-4565A_Clockwise_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M3-1524A_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M3-1537L_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M4-2156A_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M4-3434B_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M5-7650B_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M5-8291A_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M6-5441A_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M6-7036B_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M60-9327B_Counterclockwise_2019_Processed.csv\n",
      "Reading Data/Processed/Highways_England/M60-9374A_Clockwise_2019_Processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to hold the dataframes of highways england data\n",
    "england_df_list = list()\n",
    "\n",
    "# Loop through the files, sorted in alphabetical order\n",
    "# Read them into a df, make sure they are sorted by timestamp, and append to the list\n",
    "for fname in sorted(glob.glob(\"Data/Processed/Highways_England/*.csv\")):\n",
    "    print(\"Reading {}\".format(fname))\n",
    "    df = pd.read_csv(fname) #, parse_dates=['timestamp'], index_col=['timestamp'])\n",
    "    df = df.sort_values(by=\"timestamp\")\n",
    "    england_df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data/Processed/Portland/I205-101068_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/I205-101073_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/I405-100395_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/I405-100527_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/I5-100688_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/I5-100703_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/I84-101108_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/I84-101161_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/OR217-100300_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/OR217-100314_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/R2 Delta Hwy-101745_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/R2 OR18-102111_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/R2 OR18-102113_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/SR 14-102001_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/SR 14-102003_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/SR 500-1000022_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/SR 500-1000104_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/US26-100627_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Portland/US26-100650_Westbound_2019_Processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Follow the same process in this cell and the next as was done above, just for other highway systems\n",
    "portland_df_list = list()\n",
    "\n",
    "for fname in sorted(glob.glob(\"Data/Processed/Portland/*.csv\")):\n",
    "    print(\"Reading {}\".format(fname))\n",
    "    df = pd.read_csv(fname) #, parse_dates=['timestamp'], index_col=['timestamp'])\n",
    "    df = df.sort_values(by=\"timestamp\")\n",
    "    portland_df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data/Processed/Utah/I15-3103178_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/I15-749_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/I215-134_Counterclockwise_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/I215-31_Clockwise_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/I70-3103400_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/I70-3103401_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/I80-600_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/I80-667_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/I84-451_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/I84-482_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/LegacyParkway-810_Northbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/LegacyParkway-890_Southbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/US189-260_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/US189-470_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/US40-634_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/US40-635_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/US6-3103114_Eastbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/US6-3103115_Westbound_2019_Processed.csv\n",
      "Reading Data/Processed/Utah/US89-483_Northbound_2019_Processed.csv\n"
     ]
    }
   ],
   "source": [
    "utah_df_list = list()\n",
    "\n",
    "for fname in sorted(glob.glob(\"Data/Processed/Utah/*.csv\")):\n",
    "    print(\"Reading {}\".format(fname))\n",
    "    df = pd.read_csv(fname) #, parse_dates=['timestamp'], index_col=['timestamp'])\n",
    "    df = df.sort_values(by=\"timestamp\")\n",
    "    utah_df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all df lists together into one\n",
    "total_df_list = england_df_list + portland_df_list + utah_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the start and end points csv, and subtract 1 to deal with index differences between R and python\n",
    "start_end = pd.read_csv(\"start_end_points.csv\")\n",
    "start_end[\"start\"] = start_end[\"start\"] - 1\n",
    "start_end[\"end\"] = start_end[\"end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold the subset data frames (those with only 12 weeks of data per highway)\n",
    "subset_df_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each df in our original total df list\n",
    "for idx, df in enumerate(total_df_list):\n",
    "        \n",
    "    # Filter the timeframe based on the start_end_points csv files\n",
    "    subset_df = df.iloc[start_end.iloc[idx,0]:start_end.iloc[idx,1], ]\\\n",
    "    .reset_index(drop=True).reset_index(drop=False)\\\n",
    "    .rename(columns={\"index\":\"rn\"})\n",
    "    \n",
    "    # Create a new field called train_val_test to differentiate each set of data\n",
    "    subset_df[\"train_val_test\"] = np.where(subset_df[\"rn\"]<(96*7*8),\n",
    "                                           \"train\",\n",
    "                                           np.where(subset_df[\"rn\"]<(96*7*10),\n",
    "                                                    \"val\",\n",
    "                                                    \"test\"\n",
    "                                                   )\n",
    "                                       )\n",
    "    \n",
    "    # Append to list\n",
    "    subset_df_list.append(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of df's with only fields we need\n",
    "\n",
    "# Initialize empty list\n",
    "model_df_list = list()\n",
    "\n",
    "# For df in subset list\n",
    "for df in subset_df_list:\n",
    "       \n",
    "    # Extract the timestamp, the volume, and the train_val_test assignment\n",
    "    model_df = df[['timestamp', 'total_volume', \"train_val_test\"]]\\\n",
    "    .rename(columns={'timestamp':'start', 'total_volume':'target'})\n",
    "    \n",
    "    # Append this df to the new list\n",
    "    model_df_list.append(model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for progress bar:\n",
    "# https://stackoverflow.com/questions/24983493/tracking-progress-of-joblib-parallel-execution\n",
    "# This allows us to print a progress bar while running parallel loops using joblib \n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lag Emebedded Matrices for each TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20493/2159409676.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name] = df['target'].shift(n)\n"
     ]
    }
   ],
   "source": [
    "# # Lag embed the data frames and save to a list\n",
    "lag_embed_df_list = list()\n",
    "\n",
    "for df in model_df_list:\n",
    "    # For each df in our list\n",
    "    for n in range(1, 961):\n",
    "        # For each lag level, up to 960\n",
    "        # Create a new column called target-n\n",
    "        name = f\"target-{n}\"\n",
    "        # Save the target shifted n values into this colume\n",
    "        df[name] = df['target'].shift(n)\n",
    "    # Append to list\n",
    "    lag_embed_df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lag embedded list into train, val, and test lists\n",
    "\n",
    "# First, initialize empty lists for each train, val, and test\n",
    "train_df_list = list()\n",
    "val_df_list = list()\n",
    "test_df_list = list()\n",
    "\n",
    "for i in range(len(lag_embed_df_list)):\n",
    "    # For each df in our list\n",
    "    df = lag_embed_df_list[i].copy()\n",
    "\n",
    "    # Add a ts_index of i+1 to join with clustering data from R\n",
    "    df['ts_index'] = i + 1\n",
    "    \n",
    "    # Subset into train, val, and test df's based on the train_val_test_field\n",
    "    train_df = df.query(\"train_val_test == 'train'\").copy()\n",
    "    val_df = df.query(\"train_val_test=='val'\").copy()\n",
    "    test_df = df.query(\"train_val_test=='test'\").copy()\n",
    "    \n",
    "    # Append to appropriate lists\n",
    "    train_df_list.append(train_df)\n",
    "    val_df_list.append(val_df)\n",
    "    test_df_list.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all dfs from the lists together to create one full train, val, and test df\n",
    "train_df_full = pd.concat(train_df_list)\n",
    "val_df_full = pd.concat(val_df_list)\n",
    "test_df_full = pd.concat(test_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "train_df_full.drop(columns=['start', 'train_val_test'], inplace=True)\n",
    "val_df_full.drop(columns=['start', 'train_val_test'], inplace=True)\n",
    "test_df_full.drop(columns=['start', 'train_val_test'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the training and validation data together for later use\n",
    "train_val_df_full = train_df_full.append(val_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unused variables to free up memory\n",
    "del train_df_list\n",
    "del val_df_list \n",
    "del test_df_list\n",
    "del lag_embed_df_list\n",
    "del model_df_list\n",
    "del subset_df_list\n",
    "del total_df_list\n",
    "del england_df_list\n",
    "del portland_df_list\n",
    "del utah_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force garbage collection to free up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_full.shape[0]/76/96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df_full.shape[0]/76/96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df_full.shape[0]/76/96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_full.shape[0]/76/96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Train-Val Data to Validate the 840 Lag Embedding Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all lag embeddings to test - they are generally multiples of 96 (or 96*1.25), the seasonality\n",
    "lag_embed_list = [1,2,4,24,48,60,96,120,192,240,288,360,384,480,576,600,672,720,768,840,960]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train a LighGBM model given some data and a lag values\n",
    "def train_lgbm_lag(lag, data):\n",
    "    \"\"\"Function which takes in a time series lag at which to compute a model and the lag embedded \n",
    "    training dataframe to use and returns a fitted LightGBM model\n",
    "    \"\"\"\n",
    "    # Subset y and X from the data input for the given lag\n",
    "    y_train = data.iloc[:,0:(lag+1)].dropna().iloc[:,0]\n",
    "    X_train = data.iloc[:,0:(lag+1)].dropna().iloc[:,1:]\n",
    "    \n",
    "    # Create the model using boosting type goss, the true LightGBM booster and a fixed random state\n",
    "    lgbm_mod = LGBMRegressor(boosting_type='goss', random_state=54321)\n",
    "    # Fit the model\n",
    "    lgbm_mod.fit(X_train, y_train)\n",
    "    \n",
    "    # Return the fitted model\n",
    "    return lgbm_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lag Embed LGBM Models:  43%|█████████            | 9/21 [00:22<00:44,  3.68s/it]/home/ec2-user/miniconda3/envs/thesis_env/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "Lag Embed LGBM Models: 100%|████████████████████| 21/21 [02:28<00:00,  7.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# With the appropriate context manager, loop through all lag embeddings of interest and train a model\n",
    "# We do this using Parallel so that we can loop in parallel and achieve faster compute time\n",
    "with tqdm_joblib(tqdm(desc=\"Lag Embed LGBM Models\", \n",
    "                      total=len(lag_embed_list))) as progress_bar:\n",
    "    lag_embed_mods = Parallel(n_jobs=4)(delayed(train_lgbm_lag)(lag_embed_list[i], train_df_full) for i in range(len(lag_embed_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which, given a model, lag embed level, and training and validation data, returns\n",
    "# average model performance over that data\n",
    "\n",
    "def train_val_lgbm_perf_lag(model, lag, train_data, val_data):\n",
    "    \"\"\"Function which takes a trained model, the data set time series lag embedded, and a lag embedded\n",
    "    training and validation data frame and computes average model performance across each time series\n",
    "    in the data set\"\"\"\n",
    "    \n",
    "    # Set up empty lists in which we will save model peroformance metrics\n",
    "    train_rmse_sub_list = list()\n",
    "    train_mae_sub_list = list()\n",
    "    train_rmse_scaled_sub_list = list()\n",
    "    train_mae_scaled_sub_list = list()\n",
    "    \n",
    "    val_rmse_sub_list = list()\n",
    "    val_mae_sub_list = list()\n",
    "    val_rmse_scaled_sub_list = list()\n",
    "    val_mae_scaled_sub_list = list()\n",
    "    \n",
    "    # Loop through each time series index. We know we have 76 in our data set, but this could be a function input\n",
    "    # if we wanted to extend to future use cases\n",
    "    for m in range(1, 77):\n",
    "        # Subset the training data into an X and y for each ts_index and at the provided lag level\n",
    "        y_train_sub = train_data.query(\"ts_index==@m\").iloc[lag:,0]\n",
    "        X_train_sub = train_data.query(\"ts_index==@m\").iloc[lag:,0:(lag+1)].iloc[:,1:]\n",
    "        # Compute the mean of the target for scaling the perf metrics\n",
    "        train_mean_sub = np.mean(y_train_sub)\n",
    "        \n",
    "        # Do the same for the validation data\n",
    "        y_val_sub = val_data.query(\"ts_index==@m\").iloc[:,0]\n",
    "        X_val_sub = val_data.query(\"ts_index==@m\").iloc[:,1:(lag+1)]\n",
    "        val_mean_sub = np.mean(y_val_sub)\n",
    "        \n",
    "        # Make predictions with the provided model for both training and validation sets\n",
    "        train_preds_sub = model.predict(X_train_sub)\n",
    "        val_preds_sub = model.predict(X_val_sub)\n",
    "    \n",
    "        # Compute the rmse on the training data\n",
    "        train_rmse_sub = mean_squared_error(y_train_sub, train_preds_sub, squared=False)\n",
    "        # Append the rmse to the appropriate list\n",
    "        train_rmse_sub_list.append(train_rmse_sub)\n",
    "        # Append the rmse divided by the target mean to the appropriate list - this is nrmse metric\n",
    "        train_rmse_scaled_sub_list.append(train_rmse_sub/train_mean_sub)\n",
    "        \n",
    "        # Do the same for mae\n",
    "        train_mae_sub = mean_absolute_error(y_train_sub, train_preds_sub)\n",
    "        train_mae_sub_list.append(train_mae_sub)\n",
    "        train_mae_scaled_sub_list.append(train_mae_sub/train_mean_sub)\n",
    "        \n",
    "        # Do the same for validation rmse and mae\n",
    "        val_rmse_sub = mean_squared_error(y_val_sub, val_preds_sub, squared=False)\n",
    "        val_rmse_sub_list.append(val_rmse_sub)\n",
    "        val_rmse_scaled_sub_list.append(val_rmse_sub/val_mean_sub)\n",
    "        \n",
    "        val_mae_sub = mean_absolute_error(y_val_sub, val_preds_sub)\n",
    "        val_mae_sub_list.append(val_mae_sub)\n",
    "        val_mae_scaled_sub_list.append(val_mae_sub/val_mean_sub)\n",
    "    \n",
    "    # Create a dictionary to hold average model performance, computing the mean of each of the above\n",
    "    # lists of model performance \n",
    "    perf_dict = {\"train_rmse\": np.mean(train_rmse_sub_list),\n",
    "                 \"train_mae\": np.mean(train_mae_sub_list),\n",
    "                 \"train_nrmse\": np.mean(train_rmse_scaled_sub_list),\n",
    "                 \"train_smae\": np.mean(train_mae_scaled_sub_list),\n",
    "                 \n",
    "                 \"val_rmse\": np.mean(val_rmse_sub_list),\n",
    "                 \"val_mae\": np.mean(val_mae_sub_list),\n",
    "                 \"val_nrmse\": np.mean(val_rmse_scaled_sub_list),\n",
    "                 \"val_smae\": np.mean(val_mae_scaled_sub_list),\n",
    "                 \"lag\": lag\n",
    "                }\n",
    "    \n",
    "    # Return average model performance dictionary\n",
    "    return perf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lag Embed LGBM Perf: 100%|██████████████████████| 21/21 [01:59<00:00,  5.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# Again, in parallel, loop through the lag embeddings and save to a list\n",
    "with tqdm_joblib(tqdm(desc=\"Lag Embed LGBM Perf\", \n",
    "                      total=len(lag_embed_list))) as progress_bar:\n",
    "    lag_embed_perf = Parallel(n_jobs=4)(delayed(train_val_lgbm_perf_lag)(lag_embed_mods[i],\n",
    "                                                                         lag_embed_list[i], \n",
    "                                                                         train_df_full,\n",
    "                                                                         val_df_full\n",
    "                                                                        ) for i in range(len(lag_embed_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a performance data frame from the list of performance dictionaries created in the last cell\n",
    "lag_embed_perf_df = pd.DataFrame(lag_embed_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>train_nrmse</th>\n",
       "      <th>train_smae</th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_nrmse</th>\n",
       "      <th>val_smae</th>\n",
       "      <th>lag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.455627</td>\n",
       "      <td>26.581157</td>\n",
       "      <td>0.183802</td>\n",
       "      <td>0.125454</td>\n",
       "      <td>39.178294</td>\n",
       "      <td>26.560165</td>\n",
       "      <td>0.177646</td>\n",
       "      <td>0.122029</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.569484</td>\n",
       "      <td>26.173492</td>\n",
       "      <td>0.180307</td>\n",
       "      <td>0.123210</td>\n",
       "      <td>38.557208</td>\n",
       "      <td>26.254801</td>\n",
       "      <td>0.175321</td>\n",
       "      <td>0.120324</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.629872</td>\n",
       "      <td>24.732698</td>\n",
       "      <td>0.175332</td>\n",
       "      <td>0.119291</td>\n",
       "      <td>36.694092</td>\n",
       "      <td>24.828349</td>\n",
       "      <td>0.170318</td>\n",
       "      <td>0.116341</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.352365</td>\n",
       "      <td>23.090392</td>\n",
       "      <td>0.167480</td>\n",
       "      <td>0.113849</td>\n",
       "      <td>34.743085</td>\n",
       "      <td>23.320557</td>\n",
       "      <td>0.163101</td>\n",
       "      <td>0.111149</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.376031</td>\n",
       "      <td>22.418565</td>\n",
       "      <td>0.163209</td>\n",
       "      <td>0.110903</td>\n",
       "      <td>33.844988</td>\n",
       "      <td>22.709957</td>\n",
       "      <td>0.159242</td>\n",
       "      <td>0.108513</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_rmse  train_mae  train_nrmse  train_smae   val_rmse    val_mae  \\\n",
       "0   39.455627  26.581157     0.183802    0.125454  39.178294  26.560165   \n",
       "1   38.569484  26.173492     0.180307    0.123210  38.557208  26.254801   \n",
       "2   36.629872  24.732698     0.175332    0.119291  36.694092  24.828349   \n",
       "3   34.352365  23.090392     0.167480    0.113849  34.743085  23.320557   \n",
       "4   33.376031  22.418565     0.163209    0.110903  33.844988  22.709957   \n",
       "\n",
       "   val_nrmse  val_smae  lag  \n",
       "0   0.177646  0.122029    1  \n",
       "1   0.175321  0.120324    2  \n",
       "2   0.170318  0.116341    4  \n",
       "3   0.163101  0.111149   24  \n",
       "4   0.159242  0.108513   48  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag_embed_perf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a file for some inspection/plotting in R\n",
    "lag_embed_perf_df.to_csv('Results/Global/LightGBM Default/lag_model_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Global Model on Full Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our final lag value to be 840\n",
    "lag_n = 840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full X and y training set (including validation) using 840 lags\n",
    "y_train = train_val_df_full.iloc[:,0:(lag_n+1)].dropna().iloc[:,0]\n",
    "X_train = train_val_df_full.iloc[:,0:(lag_n+1)].dropna().iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='goss', random_state=54321)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit a model to these training data\n",
    "mod = LGBMRegressor(boosting_type='goss', random_state=54321)  \n",
    "mod.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Results/Global/LightGBM Default/Full/model']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to file to use later\n",
    "filename = 'Results/Global/LightGBM Default/Full/model'\n",
    "joblib.dump(mod, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute model residuals to use for bootstrapping PIs\n",
    "def compute_lgbm_residuals(mod, data, lag_n):\n",
    "    \"\"\"Function which takes inputs: a model, the data it was trained on, and a lag embedding,\n",
    "    and outputs a list of model residuals\"\"\"\n",
    "    \n",
    "    # Create X and y matrices from the data\n",
    "    X = data.iloc[:,0:(lag_n+1)].dropna().iloc[:,1:]\n",
    "    y = data.iloc[:,0:(lag_n+1)].dropna().iloc[:,0]\n",
    "    \n",
    "    # Predict the y values for the given X\n",
    "    pred = mod.predict(X)\n",
    "    \n",
    "    # Compute the residuals as the difference between true and predicted and convert to a list\n",
    "    resid = (y - pred).to_list()\n",
    "    \n",
    "    # Retrun the list of residuals\n",
    "    return resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute full model residuals using the above function\n",
    "full_mod_resid = compute_lgbm_residuals(mod, train_val_df_full, lag_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_df = pd.DataFrame({\"residual\": full_mod_resid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_df.to_csv(\"Results/Global/LightGBM Default/Full/residual.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute test preds\n",
    "def compute_lgbm_test_preds(mod, data, lag_n):\n",
    "    \"\"\"Function which takes in: a model, test data, and the lag embedding to use, and returns a df of forecasts\"\"\"\n",
    "\n",
    "    # Initialize an empty data frame to store preds\n",
    "    pred_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through each individual time series index in the data set\n",
    "    for ts_idx in data.ts_index.unique():\n",
    "        # Create the X matrix for each one\n",
    "        X = data.query(\"ts_index==@ts_idx\").iloc[:,1:(lag_n+1)].copy()\n",
    "\n",
    "        # Forecast for that X matrix\n",
    "        preds = mod.predict(X)\n",
    "        \n",
    "        # Save the results to a temp data frame\n",
    "        pred_df_sub = pd.DataFrame({\"ts_index\": ts_idx, \"test_preds\": preds})\n",
    "        \n",
    "        # Append to primary data frame\n",
    "        pred_df = pred_df.append(pred_df_sub)\n",
    "    \n",
    "    # Return df of all preds with corresponding ts_index column\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute full data test preds using above function\n",
    "full_mod_test_preds = compute_lgbm_test_preds(mod, test_df_full, lag_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute performance metrics on test data\n",
    "def compute_lgbm_test_perf(preds, data):\n",
    "    \"\"\"Function which takes inputs: a data frame of test predictions, and a test data df,\n",
    "    and which returns a data frame of model performance\"\"\"\n",
    "    \n",
    "    # Create an empty list to store model performance\n",
    "    perf_ls = list()\n",
    "    \n",
    "    # For each time series index in our data set\n",
    "    for ts_idx in data.ts_index.unique():\n",
    "        # Get the target (actual) for that index\n",
    "        y_sub = data.query(\"ts_index==@ts_idx\").iloc[:,0]\n",
    "        # Extract the corresponding forecasts\n",
    "        preds_sub = preds.query(\"ts_index==@ts_idx\").test_preds\n",
    "        \n",
    "        # Compute rmse, mae, and the mean of the true target value for those preds\n",
    "        rmse_sub = mean_squared_error(y_sub, preds_sub, squared=False)\n",
    "        mae_sub = mean_absolute_error(y_sub, preds_sub)\n",
    "        mean_sub = np.mean(y_sub)\n",
    "        \n",
    "        # Save those metrics to a dictionary\n",
    "        pred_dict = {\"rmse\": rmse_sub, \"mae\": mae_sub, \"mean\": mean_sub}\n",
    "        \n",
    "        # Append the dictionary to the list\n",
    "        perf_ls.append(pred_dict)\n",
    "        \n",
    "    # Return a data frame of model performance created from the list of dictionaries\n",
    "    return pd.DataFrame(perf_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute model perf metrics using above function\n",
    "full_mod_test_perf = compute_lgbm_test_perf(full_mod_test_preds, test_df_full)\n",
    "\n",
    "# Compute scaled performance metrics in new columns\n",
    "full_mod_test_perf['nrmse'] = full_mod_test_perf['rmse']/full_mod_test_perf['mean']\n",
    "full_mod_test_perf['smae'] = full_mod_test_perf['mae']/full_mod_test_perf['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmse      31.303586\n",
       "mae       20.764927\n",
       "mean     265.435072\n",
       "nrmse      0.142768\n",
       "smae       0.096702\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the means of model perf metrics\n",
    "full_mod_test_perf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute pred intervals with bootstrap method\n",
    "def compute_lgbm_boostrap_int(preds, resid, n_boot):\n",
    "    \"\"\"Function which takes in a model's predictions and residuals, and a number of bootstrap resamples to use,\n",
    "    and which outputs a df with pred intervals at 80% and 95%\"\"\"\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(54321)\n",
    "    np.random.seed(54321)\n",
    "    \n",
    "    # Create empty columns in the pred df to store the PIs\n",
    "    preds['lo_95'] = np.nan\n",
    "    preds['hi_95'] = np.nan\n",
    "    preds['lo_80'] = np.nan\n",
    "    preds['hi_80'] = np.nan\n",
    "    \n",
    "    # For each row in the pred df\n",
    "    for n in range(preds.shape[0]):\n",
    "        # Sample with replacement n_boot times from the residuals\n",
    "        resid_boot = np.random.choice(resid, size=n_boot, replace=True)\n",
    "        # Extract the forecast value for that row\n",
    "        pred_n = preds.iloc[n, :].test_preds\n",
    "        # Add the residual vector to the forecast value\n",
    "        pred_n_boot = resid_boot + pred_n\n",
    "        \n",
    "        # Compute quantiles of this residual+forecast vector\n",
    "        percent_95_lo = np.percentile(pred_n_boot, 2.5)\n",
    "        percent_95_hi = np.percentile(pred_n_boot, 97.5)\n",
    "        \n",
    "        percent_80_lo = np.percentile(pred_n_boot, 10)\n",
    "        percent_80_hi = np.percentile(pred_n_boot, 90)\n",
    "        \n",
    "        # Save these quantiles to the appropriate df column\n",
    "        preds.iloc[n, 2] = percent_95_lo\n",
    "        preds.iloc[n, 3] = percent_95_hi\n",
    "        preds.iloc[n, 4] = percent_80_lo\n",
    "        preds.iloc[n, 5] = percent_80_hi\n",
    "    \n",
    "    # Return the updated preds data frame\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PIs with 1000 bootstrap samples\n",
    "full_mod_boot_ints = compute_lgbm_boostrap_int(full_mod_test_preds, full_mod_resid, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the true values into their own df column\n",
    "full_mod_boot_ints['actual'] = test_df_full.iloc[:,0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_index</th>\n",
       "      <th>test_preds</th>\n",
       "      <th>lo_95</th>\n",
       "      <th>hi_95</th>\n",
       "      <th>lo_80</th>\n",
       "      <th>hi_80</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>325.434391</td>\n",
       "      <td>268.889593</td>\n",
       "      <td>387.638256</td>\n",
       "      <td>299.369989</td>\n",
       "      <td>354.752278</td>\n",
       "      <td>320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>326.442850</td>\n",
       "      <td>269.158972</td>\n",
       "      <td>388.523561</td>\n",
       "      <td>296.301457</td>\n",
       "      <td>355.085566</td>\n",
       "      <td>339.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>343.486459</td>\n",
       "      <td>282.590215</td>\n",
       "      <td>411.880372</td>\n",
       "      <td>312.747231</td>\n",
       "      <td>372.239508</td>\n",
       "      <td>349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>347.672272</td>\n",
       "      <td>287.013922</td>\n",
       "      <td>412.413097</td>\n",
       "      <td>319.303457</td>\n",
       "      <td>378.704519</td>\n",
       "      <td>343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>343.833946</td>\n",
       "      <td>278.444722</td>\n",
       "      <td>404.274194</td>\n",
       "      <td>310.302711</td>\n",
       "      <td>373.207130</td>\n",
       "      <td>343.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ts_index  test_preds       lo_95       hi_95       lo_80       hi_80  \\\n",
       "0         1  325.434391  268.889593  387.638256  299.369989  354.752278   \n",
       "1         1  326.442850  269.158972  388.523561  296.301457  355.085566   \n",
       "2         1  343.486459  282.590215  411.880372  312.747231  372.239508   \n",
       "3         1  347.672272  287.013922  412.413097  319.303457  378.704519   \n",
       "4         1  343.833946  278.444722  404.274194  310.302711  373.207130   \n",
       "\n",
       "   actual  \n",
       "0   320.0  \n",
       "1   339.0  \n",
       "2   349.0  \n",
       "3   343.0  \n",
       "4   343.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_mod_boot_ints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute the interval score\n",
    "def interval_score(true_values, lower, upper, interval_range):\n",
    "    \"\"\" Function which takes in the true values, the upper and lower bounds of PIs, and the PI level (e.g., 90%)\n",
    "        and from these inputs, computes the interval score for each prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute alpha from the interval range\n",
    "    alpha = 1-interval_range\n",
    "    \n",
    "    # Save the upper, lower, and true_values as numpy arrays for computation purposes\n",
    "    upper = np.array(upper)\n",
    "    lower = np.array(lower)\n",
    "    true_values = np.array(true_values)\n",
    "    \n",
    "    # Compute the lower component of the interval score - just a boolean for true below interval\n",
    "    def lower_ind(true,low):\n",
    "        if true<low:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    # Computer the upper component of the interval score - similar boolean for true above interval\n",
    "    def upper_ind(true,up):\n",
    "        if true>up:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    # Computer the actual score for each obsveration - formula here: https://epiforecasts.io/scoringutils/reference/interval_score.html\n",
    "    scores = (upper-lower) + (2/alpha)*(lower-true_values)*(lower > true_values) + (2/alpha)*(true_values-upper)*(true_values > upper)\n",
    "    \n",
    "    # Return the scores array\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 95% and 80% PI scores using the above function as new data frame columns\n",
    "full_mod_boot_ints['int_95_score'] = interval_score(full_mod_boot_ints.actual, \n",
    "                                                    full_mod_boot_ints.lo_95,\n",
    "                                                    full_mod_boot_ints.hi_95,\n",
    "                                                    0.95)\n",
    "                                                    \n",
    "full_mod_boot_ints['int_80_score'] = interval_score(full_mod_boot_ints.actual, \n",
    "                                                    full_mod_boot_ints.lo_80,\n",
    "                                                    full_mod_boot_ints.hi_80,\n",
    "                                                    0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ts_index         38.500000\n",
       "test_preds      265.875193\n",
       "lo_95           201.657780\n",
       "hi_95           332.379614\n",
       "lo_80           235.797174\n",
       "hi_80           296.879261\n",
       "actual          265.435072\n",
       "int_95_score    228.236174\n",
       "int_80_score    124.369523\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the means of the interval scores\n",
    "full_mod_boot_ints.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mod_boot_ints_group = full_mod_boot_ints.groupby(\"ts_index\")\\\n",
    ".agg({'int_95_score':'mean', 'int_80_score':'mean', 'actual':'mean'}).reset_index()\n",
    "\n",
    "full_mod_boot_ints_group['int_95_score_scaled'] = full_mod_boot_ints_group['int_95_score']/full_mod_boot_ints_group['actual']\n",
    "full_mod_boot_ints_group['int_80_score_scaled'] = full_mod_boot_ints_group['int_80_score']/full_mod_boot_ints_group['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int_80_score_scaled    0.625180\n",
       "int_95_score_scaled    1.166136\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_mod_boot_ints_group[['int_80_score_scaled', 'int_95_score_scaled']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "full_mod_boot_ints.to_csv(\"Results/Global/LightGBM Default/Full/test_pred_intervals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test on Random Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in cluster assignments for random clusters\n",
    "rand_clust = pd.read_csv(\"Results/Clustering/Random/random_clustering_assign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a field called cluster with the cluster assignments (for simplicity later on)\n",
    "rand_clust['cluster'] = rand_clust['random_clust_assign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a LightGBM model on data with a cluster assignment\n",
    "def train_lgbm_clust(data, cluster_no, lag_n):\n",
    "    \"\"\"Function takes in: data to train on, the cluster number to use, and the lag_n lag embedding to use.\n",
    "    Function returns the trained model. \"\"\"\n",
    "    \n",
    "    # Create X and y to train model by filtering to the appropriate cluster number and lag_embedding\n",
    "    X_train = data.query(\"cluster==@cluster_no\").copy().iloc[:,0:(lag_n+1)].dropna().iloc[:,1:]\n",
    "    y_train = data.query(\"cluster==@cluster_no\").copy().iloc[:,0:(lag_n+1)].dropna().iloc[:,0]\n",
    "    \n",
    "    # Create and train the model\n",
    "    mod = LGBMRegressor(boosting_type='goss', random_state=54321)  \n",
    "    mod.fit(X_train,y_train)\n",
    "    \n",
    "    # Return the fitted model\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the training and test data with the cluster assignements (essentially join) on ts_index\n",
    "train_val_df_full_rand_clust = train_val_df_full.merge(rand_clust, on=\"ts_index\")\n",
    "test_df_full_rand_clust = test_df_full.merge(rand_clust, on=\"ts_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random Cluster LGBM Models: 100%|█████████████████| 4/4 [00:36<00:00,  9.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Parallel loop through the cluster assignments and create the models\n",
    "with tqdm_joblib(tqdm(desc=\"Random Cluster LGBM Models\", \n",
    "                      total=len(rand_clust.cluster.unique()))) as progress_bar:\n",
    "    rand_clust_mods = Parallel(n_jobs=2)(delayed(train_lgbm_clust)(train_val_df_full_rand_clust, \n",
    "                                                                   i,\n",
    "                                                                   lag_n\n",
    "                                                                  ) for i in range(1, len(rand_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models to the appropriate directory using joblib.dump\n",
    "for clust_no in range(1, len(rand_clust.cluster.unique())+1):\n",
    "    filename = f'Results/Global/LightGBM Default/Random Cluster/model_{clust_no}'\n",
    "    joblib.dump(rand_clust_mods[clust_no-1], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_clust_mods = list()\n",
    "for mod_no in range(1, len(rand_clust.cluster.unique())+1):\n",
    "    rand_clust_mods.append(joblib.load(f'Results/Global/LightGBM Default/Random Cluster/model_{mod_no}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute model residuals for clustered data\n",
    "def compute_lgbm_resid_clust(mod, cluster_no, data, lag_n):\n",
    "    \"\"\"Function which takes in a trained model, cluster number, training data, and lag embedding level\n",
    "    and which returns a list of model residuals\"\"\"\n",
    "    \n",
    "    # Create X and y from the data\n",
    "    X_train = data.query(\"cluster==@cluster_no\").copy().iloc[:,0:(lag_n+1)].dropna().iloc[:,1:]\n",
    "    y_train = data.query(\"cluster==@cluster_no\").copy().iloc[:,0:(lag_n+1)].dropna().iloc[:,0]\n",
    "    \n",
    "    # Make predictions\n",
    "    pred = mod.predict(X_train)\n",
    "    \n",
    "    # Compute residuals and convert to list\n",
    "    resid = (y_train - pred).to_list()\n",
    "    \n",
    "    # Return list of residuals\n",
    "    return resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random Cluster LGBM Residuals: 100%|██████████████| 4/4 [00:11<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# Parallel loop through models and compute residuals for each\n",
    "with tqdm_joblib(tqdm(desc=\"Random Cluster LGBM Residuals\", \n",
    "                      total=len(rand_clust.cluster.unique()))) as progress_bar:\n",
    "    rand_clust_residuals = Parallel(n_jobs=2)(delayed(compute_lgbm_resid_clust)(rand_clust_mods[i-1], \n",
    "                                                                                i,\n",
    "                                                                                train_val_df_full_rand_clust,\n",
    "                                                                                lag_n\n",
    "                                                                               ) for i in range(1, len(rand_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_clust_res_df = pd.DataFrame({'cluster': list({(i+1): rand_clust_residuals[i] for i in range(len(rand_clust_residuals))}.keys()),\n",
    "                                  'residual': list({(i+1): rand_clust_residuals[i] for i in range(len(rand_clust_residuals))}.values())})\n",
    "\n",
    "rand_clust_res_df.to_csv(\"Results/Global/LightGBM Default/Random Cluster/residual.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute clustered test preds\n",
    "def compute_lgbm_test_preds_clust(mod, cluster_no, data, lag_n):\n",
    "    \"\"\"Function which takes inputs: a trained model, a cluster number, test data, and lag embedding\n",
    "    and which returns a df of model predictions on the test data\"\"\"\n",
    "    \n",
    "    # Start by creating an empty data frame\n",
    "    pred_df = pd.DataFrame()\n",
    "    \n",
    "    # Subset the test data to the provided cluster number\n",
    "    data = data.query(\"cluster==@cluster_no\").copy()\n",
    "    \n",
    "    # Loop through all the time series in the cluster\n",
    "    for ts_idx in data.ts_index.unique():\n",
    "        \n",
    "        # Filter to each ts_index\n",
    "        X = data.query(\"ts_index==@ts_idx\").iloc[:,1:(lag_n+1)].copy()\n",
    "        \n",
    "        # Compute predictions for that time series\n",
    "        preds = mod.predict(X)\n",
    "        \n",
    "        # Save the resulds to a temp data frame\n",
    "        pred_df_sub = pd.DataFrame({\"ts_index\": ts_idx, \"test_preds\": preds})\n",
    "        \n",
    "        # Append to the primary data frame\n",
    "        pred_df = pred_df.append(pred_df_sub)\n",
    "    \n",
    "    # Return the data frame of model predictions\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random Cluster LGBM Test Preds: 100%|█████████████| 4/4 [00:04<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop through all the clusters and call the function above to compute test preds\n",
    "# Again, we use joblib to do this in a parallel fashion and we use the tdqm_joblib function to print a progress bar\n",
    "with tqdm_joblib(tqdm(desc=\"Random Cluster LGBM Test Preds\", \n",
    "                      total=len(rand_clust.cluster.unique()))) as progress_bar:\n",
    "    rand_clust_test_preds = Parallel(n_jobs=2)(delayed(compute_lgbm_test_preds_clust)(rand_clust_mods[i-1], \n",
    "                                                                                      i,\n",
    "                                                                                      test_df_full_rand_clust,\n",
    "                                                                                      lag_n\n",
    "                                                                                     ) for i in range(1, len(rand_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty data frame\n",
    "rand_clust_test_preds_df = pd.DataFrame()\n",
    "\n",
    "# For each data frame in the list of prediction data frames\n",
    "for clust_test_pred_df in rand_clust_test_preds:\n",
    "    # Append to the newly created data frame\n",
    "    rand_clust_test_preds_df = rand_clust_test_preds_df.append(clust_test_pred_df)\n",
    "\n",
    "# Compute model performance for the clustered predictions\n",
    "rand_clust_test_perf = compute_lgbm_test_perf(rand_clust_test_preds_df,\n",
    "                                              test_df_full_rand_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.586654</td>\n",
       "      <td>18.727047</td>\n",
       "      <td>259.136161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.986096</td>\n",
       "      <td>17.794514</td>\n",
       "      <td>195.970982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.460902</td>\n",
       "      <td>17.215643</td>\n",
       "      <td>207.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.779658</td>\n",
       "      <td>24.815757</td>\n",
       "      <td>395.383929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45.574871</td>\n",
       "      <td>30.017939</td>\n",
       "      <td>445.119048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rmse        mae        mean\n",
       "0  25.586654  18.727047  259.136161\n",
       "1  25.986096  17.794514  195.970982\n",
       "2  24.460902  17.215643  207.250000\n",
       "3  38.779658  24.815757  395.383929\n",
       "4  45.574871  30.017939  445.119048"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_clust_test_perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute columns for normalized performance\n",
    "rand_clust_test_perf['nrmse'] = rand_clust_test_perf['rmse']/rand_clust_test_perf['mean']\n",
    "rand_clust_test_perf['smae'] = rand_clust_test_perf['mae']/rand_clust_test_perf['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmse      31.510284\n",
       "mae       20.919646\n",
       "mean     265.435072\n",
       "nrmse      0.142858\n",
       "smae       0.096716\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the means\n",
    "rand_clust_test_perf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random Cluster LGBM Residuals: 100%|██████████████| 4/4 [03:17<00:00, 49.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Parallel loop through clusters and compute bootstrap PIs - save dfs to a list\n",
    "with tqdm_joblib(tqdm(desc=\"Random Cluster LGBM Residuals\", \n",
    "                      total=len(rand_clust.cluster.unique()))) as progress_bar:\n",
    "    rand_clust_test_pred_int = Parallel(n_jobs=4)(delayed(compute_lgbm_boostrap_int)(rand_clust_test_preds[i-1], \n",
    "                                                                                     rand_clust_residuals[i-1],\n",
    "                                                                                     1000) for i in range(1, len(rand_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster\n",
    "for n in range(1, len(rand_clust_test_pred_int)+1):\n",
    "    \n",
    "    # Get the actual y values for that cluster\n",
    "    y_actual_sub = test_df_full_rand_clust.query(\"cluster==@n\").copy().iloc[:,0].to_list()\n",
    "    \n",
    "    # Add the actual values to the data frame of PIs\n",
    "    rand_clust_test_pred_int[n-1]['actual'] = y_actual_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, for each cluster\n",
    "for m in range(len(rand_clust_test_pred_int)):\n",
    "    # Compute the interval scores at 95% and 80% as new df columns\n",
    "    rand_clust_test_pred_int[m]['int_95_score'] = interval_score(rand_clust_test_pred_int[m]['actual'],\n",
    "                                                                 rand_clust_test_pred_int[m]['lo_95'],\n",
    "                                                                 rand_clust_test_pred_int[m]['hi_95'],\n",
    "                                                                 0.95\n",
    "                                                                )\n",
    "\n",
    "    rand_clust_test_pred_int[m]['int_80_score'] = interval_score(rand_clust_test_pred_int[m]['actual'],\n",
    "                                                                 rand_clust_test_pred_int[m]['lo_80'],\n",
    "                                                                 rand_clust_test_pred_int[m]['hi_80'],\n",
    "                                                                 0.80\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all PI data frames into one by first creating an empty df and then looping through the list of PI dfs\n",
    "rand_clust_test_pred_int_df = pd.concat(rand_clust_test_pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ts_index         38.500000\n",
       "test_preds      265.759651\n",
       "lo_95           206.032728\n",
       "hi_95           327.081947\n",
       "lo_80           236.327738\n",
       "hi_80           296.040683\n",
       "actual          265.435072\n",
       "int_95_score    226.430963\n",
       "int_80_score    124.122031\n",
       "dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_clust_test_pred_int_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_clust_test_pred_int_df_grouped = rand_clust_test_pred_int_df.groupby(\"ts_index\")\\\n",
    ".agg({'int_95_score':'mean', 'int_80_score':'mean', 'actual':'mean'}).reset_index()\n",
    "\n",
    "rand_clust_test_pred_int_df_grouped['int_95_score_scaled'] = rand_clust_test_pred_int_df_grouped['int_95_score']/rand_clust_test_pred_int_df_grouped['actual']\n",
    "rand_clust_test_pred_int_df_grouped['int_80_score_scaled'] = rand_clust_test_pred_int_df_grouped['int_80_score']/rand_clust_test_pred_int_df_grouped['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int_80_score_scaled    0.603182\n",
       "int_95_score_scaled    1.079982\n",
       "dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_clust_test_pred_int_df_grouped[['int_80_score_scaled', 'int_95_score_scaled']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PI df to csv\n",
    "rand_clust_test_pred_int_df.to_csv(\"Results/Global/LightGBM Default/Random Cluster/test_pred_intervals.csv\", \n",
    "                                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test per Highway System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete variables no longer needed\n",
    "del rand_clust_test_pred_int_df\n",
    "del rand_clust_test_pred_int\n",
    "del y_actual_sub\n",
    "del rand_clust_test_perf\n",
    "del rand_clust_test_preds\n",
    "del rand_clust_residuals\n",
    "del rand_clust_mods\n",
    "del train_val_df_full_rand_clust\n",
    "del test_df_full_rand_clust\n",
    "del rand_clust\n",
    "del resid_df\n",
    "del rand_clust_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage collect\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame of highway system clusters based on the number of files we have for each highway system\n",
    "highway_system_clust = pd.DataFrame({\"ts_index\": np.arange(1, 77),\n",
    "                                    \"cluster\": [1]*38 + [2]*19 + [3]*19}\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the training and test data with the cluster assignments \n",
    "train_val_df_full_highway_clust = train_val_df_full.merge(highway_system_clust, on=\"ts_index\")\n",
    "test_df_full_highway_clust = test_df_full.merge(highway_system_clust, on=\"ts_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Highway System LGBM Models: 100%|█████████████████| 3/3 [00:35<00:00, 11.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# Parallel loop through the clusters to train the models and save trained models to a list\n",
    "with tqdm_joblib(tqdm(desc=\"Highway System LGBM Models\", \n",
    "                      total=len(highway_system_clust.cluster.unique()))) as progress_bar:\n",
    "    highway_clust_mods = Parallel(n_jobs=3)(delayed(train_lgbm_clust)(train_val_df_full_highway_clust, \n",
    "                                                                      i,\n",
    "                                                                      lag_n\n",
    "                                                                     ) for i in range(1, len(highway_system_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to files using joblib.dump\n",
    "for clust_no in range(1, len(highway_system_clust.cluster.unique())+1):\n",
    "    filename = f'Results/Global/LightGBM Default/Highway System/model_{clust_no}'\n",
    "    joblib.dump(highway_clust_mods[clust_no-1], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to files using joblib.dump\n",
    "highway_clust_mods = list()\n",
    "for clust_no in range(1, len(highway_system_clust.cluster.unique())+1):\n",
    "    filename = f'Results/Global/LightGBM Default/Highway System/model_{clust_no}'\n",
    "    highway_clust_mods.append(joblib.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Highway System LGBM Residuals: 100%|██████████████| 3/3 [00:09<00:00,  3.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the clusters and compute the residuals - save to a list\n",
    "with tqdm_joblib(tqdm(desc=\"Highway System LGBM Residuals\", \n",
    "                      total=len(highway_system_clust.cluster.unique()))) as progress_bar:\n",
    "    highway_clust_residuals = Parallel(n_jobs=3)(delayed(compute_lgbm_resid_clust)(highway_clust_mods[i-1],\n",
    "                                                                                   i,\n",
    "                                                                                   train_val_df_full_highway_clust,\n",
    "                                                                                   lag_n\n",
    "                                                                                  ) for i in range(1, len(highway_system_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_res_df = pd.DataFrame({'cluster': list({(i+1): highway_clust_residuals[i] for i in range(len(highway_clust_residuals))}.keys()),\n",
    "                               'residual': list({(i+1): highway_clust_residuals[i] for i in range(len(highway_clust_residuals))}.values())})\n",
    "\n",
    "highway_res_df.to_csv(\"Results/Global/LightGBM Default/Highway System/residual.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[-15.010870531107003, -1.5236122590959553, -49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[28.376722820189144, -13.392492828881444, 19.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[-47.25459788313378, -15.637389705863654, -0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                           residual\n",
       "0        1  [-15.010870531107003, -1.5236122590959553, -49...\n",
       "1        2  [28.376722820189144, -13.392492828881444, 19.8...\n",
       "2        3  [-47.25459788313378, -15.637389705863654, -0.1..."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highway_res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Highway System LGBM Test Preds: 100%|█████████████| 3/3 [00:03<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the clusters and compute test predictions and save df's to a list\n",
    "with tqdm_joblib(tqdm(desc=\"Highway System LGBM Test Preds\", \n",
    "                      total=len(highway_system_clust.cluster.unique()))) as progress_bar:\n",
    "    highway_clust_test_preds = Parallel(n_jobs=3)(delayed(compute_lgbm_test_preds_clust)(highway_clust_mods[i-1],\n",
    "                                                                                         i,\n",
    "                                                                                         test_df_full_highway_clust,\n",
    "                                                                                         lag_n\n",
    "                                                                                        ) for i in range(1, len(highway_system_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all data frames of test preds into one\n",
    "highway_clust_test_preds_df = pd.concat(highway_clust_test_preds)\n",
    "\n",
    "# Compute test set model performance\n",
    "highway_clust_test_perf = compute_lgbm_test_perf(highway_clust_test_preds_df,\n",
    "                                                 test_df_full_highway_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scaled metrics a new df columns\n",
    "highway_clust_test_perf['nrmse'] = highway_clust_test_perf['rmse']/highway_clust_test_perf['mean']\n",
    "highway_clust_test_perf['smae'] = highway_clust_test_perf['mae']/highway_clust_test_perf['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmse      31.133296\n",
       "mae       20.659435\n",
       "mean     265.435072\n",
       "nrmse      0.141038\n",
       "smae       0.095516\n",
       "dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the mean of model performance\n",
    "highway_clust_test_perf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Highway System LGBM PI: 100%|████████████████████| 3/3 [11:55<00:00, 238.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the clusters and compute test set PIs, saving to a list of dataframes \n",
    "with tqdm_joblib(tqdm(desc=\"Highway System LGBM PI\", \n",
    "                      total=len(highway_system_clust.cluster.unique()))) as progress_bar:\n",
    "    highway_clust_test_pred_int = Parallel(n_jobs=3)(delayed(compute_lgbm_boostrap_int)(highway_clust_test_preds[i-1],\n",
    "                                                                                        highway_clust_residuals[i-1],\n",
    "                                                                                        1000) for i in range(1, len(highway_system_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster\n",
    "for n in range(1, len(highway_clust_test_pred_int)+1):\n",
    "    \n",
    "    # Get the true values\n",
    "    y_actual_sub = test_df_full_highway_clust.query(\"cluster==@n\").copy().iloc[:,0].to_list()\n",
    "    \n",
    "    # Add these as a column to the corresponding df of test pred PIs\n",
    "    highway_clust_test_pred_int[n-1]['actual'] = y_actual_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster's PIs\n",
    "for m in range(len(highway_clust_test_pred_int)):\n",
    "    # Computer the 80% and 95% interval scores\n",
    "    highway_clust_test_pred_int[m]['int_95_score'] = interval_score(highway_clust_test_pred_int[m]['actual'],\n",
    "                                                                    highway_clust_test_pred_int[m]['lo_95'],\n",
    "                                                                    highway_clust_test_pred_int[m]['hi_95'],\n",
    "                                                                    0.95\n",
    "                                                                   )\n",
    "\n",
    "    highway_clust_test_pred_int[m]['int_80_score'] = interval_score(highway_clust_test_pred_int[m]['actual'],\n",
    "                                                                    highway_clust_test_pred_int[m]['lo_80'],\n",
    "                                                                    highway_clust_test_pred_int[m]['hi_80'],\n",
    "                                                                    0.80\n",
    "                                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one data frame from all test pred PI data frames\n",
    "highway_clust_test_pred_int_df = pd.concat(highway_clust_test_pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ts_index         38.500000\n",
       "test_preds      265.698815\n",
       "lo_95           205.355121\n",
       "hi_95           328.253474\n",
       "lo_80           236.322297\n",
       "hi_80           296.228465\n",
       "actual          265.435072\n",
       "int_95_score    221.270179\n",
       "int_80_score    122.291010\n",
       "dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the mean interval scores\n",
    "highway_clust_test_pred_int_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_clust_test_pred_int_df_grouped = highway_clust_test_pred_int_df.groupby(\"ts_index\")\\\n",
    ".agg({'int_95_score':'mean', 'int_80_score':'mean', 'actual':'mean'}).reset_index()\n",
    "\n",
    "highway_clust_test_pred_int_df_grouped['int_95_score_scaled'] = highway_clust_test_pred_int_df_grouped['int_95_score']/highway_clust_test_pred_int_df_grouped['actual']\n",
    "highway_clust_test_pred_int_df_grouped['int_80_score_scaled'] = highway_clust_test_pred_int_df_grouped['int_80_score']/highway_clust_test_pred_int_df_grouped['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int_80_score_scaled    0.589854\n",
       "int_95_score_scaled    1.058165\n",
       "dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highway_clust_test_pred_int_df_grouped[['int_80_score_scaled', 'int_95_score_scaled']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the PI data frame to a file\n",
    "highway_clust_test_pred_int_df.to_csv(\"Results/Global/LightGBM Default/Highway System/test_pred_intervals.csv\", \n",
    "                                      index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test - Catch22 KMeans Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecessary variables to save on RAM\n",
    "del highway_clust_test_pred_int_df\n",
    "del highway_clust_test_pred_int\n",
    "del highway_res_df\n",
    "del y_actual_sub\n",
    "del highway_clust_test_perf\n",
    "del highway_clust_test_preds\n",
    "del highway_clust_residuals\n",
    "del highway_clust_mods\n",
    "del train_val_df_full_highway_clust\n",
    "del test_df_full_highway_clust\n",
    "del highway_system_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force garabage collect\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in cluster assignments from Catch22-based clusters\n",
    "catch22_clust = pd.read_csv(\"Results/Clustering/KMeans/kmeans_catch22_clustering_assign.csv\")\n",
    "# Rename the field to \"cluster\" to match expectations from above functions\n",
    "catch22_clust['cluster'] = catch22_clust['kmeans_catch22_clust_assign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the training and test data with the cluster assignments\n",
    "train_val_df_full_catch22_clust = train_val_df_full.merge(catch22_clust, on=\"ts_index\")\n",
    "test_df_full_catch22_clust = test_df_full.merge(catch22_clust, on=\"ts_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Catch22 LGBM Models: 100%|████████████████████████| 5/5 [00:36<00:00,  7.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# For each cluster, loop through in a parallel way and train a light gbm model - save to list of models\n",
    "with tqdm_joblib(tqdm(desc=\"Catch22 LGBM Models\", \n",
    "                      total=len(catch22_clust.cluster.unique()))) as progress_bar:\n",
    "    catch22_clust_mods = Parallel(n_jobs=3)(delayed(train_lgbm_clust)(train_val_df_full_catch22_clust, \n",
    "                                                                      i,\n",
    "                                                                      lag_n\n",
    "                                                                     ) for i in range(1, len(catch22_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model, save the models to files using joblib.dump for future use\n",
    "for clust_no in range(1, len(catch22_clust.cluster.unique())+1):\n",
    "    filename = f'Results/Global/LightGBM Default/Catch22 KMeans/model_{clust_no}'\n",
    "    joblib.dump(catch22_clust_mods[clust_no-1], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model, save the models to files using joblib.dump for future use\n",
    "catch22_clust_mods = list()\n",
    "\n",
    "for clust_no in range(1, len(catch22_clust.cluster.unique())+1):\n",
    "    filename = f'Results/Global/LightGBM Default/Catch22 KMeans/model_{clust_no}'\n",
    "    catch22_clust_mods.append(joblib.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Catch22 LGBM Residuals: 100%|█████████████████████| 5/5 [00:09<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# For each model, loop through in a parallel fashion and compute model residuals. Save each model's residuals\n",
    "# to the list called catch22_clust_residuals\n",
    "with tqdm_joblib(tqdm(desc=\"Catch22 LGBM Residuals\", \n",
    "                      total=len(catch22_clust.cluster.unique()))) as progress_bar:\n",
    "    catch22_clust_residuals = Parallel(n_jobs=3)(delayed(compute_lgbm_resid_clust)(catch22_clust_mods[i-1],\n",
    "                                                                                   i,\n",
    "                                                                                   train_val_df_full_catch22_clust,\n",
    "                                                                                   lag_n\n",
    "                                                                                  ) for i in range(1, len(catch22_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.9643008794417725, -1.2605923422304315, -7.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[3.664500161840323, -19.625317840160847, -7.62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.10184140214194581, -1.1317853952585195, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[-6.5148478239201495, 23.22633531123006, -38.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[19.04927975710811, 19.1956187718352, 9.408641...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                           residual\n",
       "0        1  [0.9643008794417725, -1.2605923422304315, -7.1...\n",
       "1        2  [3.664500161840323, -19.625317840160847, -7.62...\n",
       "2        3  [-0.10184140214194581, -1.1317853952585195, -0...\n",
       "3        4  [-6.5148478239201495, 23.22633531123006, -38.5...\n",
       "4        5  [19.04927975710811, 19.1956187718352, 9.408641..."
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catch22_res_df = pd.DataFrame({'cluster': list({(i+1): catch22_clust_residuals[i] for i in range(len(catch22_clust_residuals))}.keys()),\n",
    "'residual': list({(i+1): catch22_clust_residuals[i] for i in range(len(catch22_clust_residuals))}.values())})\n",
    "\n",
    "catch22_res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch22_res_df.to_csv(\"Results/Global/LightGBM Default/Catch22 KMeans/residual.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Catch22 LGBM Test Preds: 100%|████████████████████| 5/5 [00:03<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Parallel loop through each model and compute predictions on the test save. Save the df of preds to a list\n",
    "with tqdm_joblib(tqdm(desc=\"Catch22 LGBM Test Preds\", \n",
    "                      total=len(catch22_clust.cluster.unique()))) as progress_bar:\n",
    "    catch22_clust_test_preds = Parallel(n_jobs=3)(delayed(compute_lgbm_test_preds_clust)(catch22_clust_mods[i-1],\n",
    "                                                                                         i,\n",
    "                                                                                         test_df_full_catch22_clust,\n",
    "                                                                                         lag_n\n",
    "                                                                                        ) for i in range(1, len(catch22_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty data frame\n",
    "catch22_clust_test_preds_df = pd.concat(catch22_clust_test_preds)\n",
    "\n",
    "# Use this data frame of all test preds to compute test pred performance\n",
    "catch22_clust_test_perf = compute_lgbm_test_perf(catch22_clust_test_preds_df,\n",
    "                                                 test_df_full_catch22_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created columns of normalized rmse and scaled mae in our performance data frame\n",
    "catch22_clust_test_perf['nrmse'] = catch22_clust_test_perf['rmse']/catch22_clust_test_perf['mean']\n",
    "catch22_clust_test_perf['smae'] = catch22_clust_test_perf['mae']/catch22_clust_test_perf['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmse      31.290422\n",
       "mae       20.745887\n",
       "mean     265.435072\n",
       "nrmse      0.140943\n",
       "smae       0.095138\n",
       "dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the means of the performance metrics\n",
    "catch22_clust_test_perf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Catch22 LGBM PI: 100%|███████████████████████████| 5/5 [11:15<00:00, 135.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# For each set of residuals and test preds, create bootstrap prediction intervals via parallel for loop\n",
    "# These intervals are saved in a df, and the output here is a list of those data frames\n",
    "with tqdm_joblib(tqdm(desc=\"Catch22 LGBM PI\", \n",
    "                      total=len(catch22_clust.cluster.unique()))) as progress_bar:\n",
    "    catch22_clust_test_pred_int = Parallel(n_jobs=3)(delayed(compute_lgbm_boostrap_int)(catch22_clust_test_preds[i-1],\n",
    "                                                                                        catch22_clust_residuals[i-1],\n",
    "                                                                                        1000) for i in range(1, len(catch22_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By looping through each individual cluster\n",
    "for n in range(1, len(catch22_clust_test_pred_int)+1):\n",
    "    # Extract the true values for the target variable for that cluster\n",
    "    y_actual_sub = test_df_full_catch22_clust.query(\"cluster==@n\").copy().iloc[:,0].to_list()\n",
    "    # Add those to the data frame of prediction intervals for that cluster\n",
    "    catch22_clust_test_pred_int[n-1]['actual'] = y_actual_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each data frame of prediction intervals \n",
    "for m in range(len(catch22_clust_test_pred_int)):\n",
    "    # Computer the 95% PI score\n",
    "    catch22_clust_test_pred_int[m]['int_95_score'] = interval_score(catch22_clust_test_pred_int[m]['actual'],\n",
    "                                                                    catch22_clust_test_pred_int[m]['lo_95'],\n",
    "                                                                    catch22_clust_test_pred_int[m]['hi_95'],\n",
    "                                                                    0.95\n",
    "                                                                   )\n",
    "\n",
    "    # Compute the 80% PI score\n",
    "    catch22_clust_test_pred_int[m]['int_80_score'] = interval_score(catch22_clust_test_pred_int[m]['actual'],\n",
    "                                                                    catch22_clust_test_pred_int[m]['lo_80'],\n",
    "                                                                    catch22_clust_test_pred_int[m]['hi_80'],\n",
    "                                                                    0.80\n",
    "                                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all PI data frames into one data frame\n",
    "catch22_clust_test_pred_int_df = pd.concat(catch22_clust_test_pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ts_index         38.500000\n",
       "test_preds      265.589308\n",
       "lo_95           207.437175\n",
       "hi_95           325.279309\n",
       "lo_80           236.197707\n",
       "hi_80           295.728206\n",
       "actual          265.435072\n",
       "int_95_score    217.156534\n",
       "int_80_score    120.536336\n",
       "dtype: float64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the means of the PI scores\n",
    "catch22_clust_test_pred_int_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch22_clust_test_pred_int_df_grouped = catch22_clust_test_pred_int_df.groupby(\"ts_index\")\\\n",
    ".agg({'int_95_score': 'mean', 'int_80_score':'mean', 'actual':'mean'}).reset_index()\n",
    "\n",
    "catch22_clust_test_pred_int_df_grouped['int_95_score_scaled'] = catch22_clust_test_pred_int_df_grouped['int_95_score']/catch22_clust_test_pred_int_df_grouped['actual']\n",
    "catch22_clust_test_pred_int_df_grouped['int_80_score_scaled'] = catch22_clust_test_pred_int_df_grouped['int_80_score']/catch22_clust_test_pred_int_df_grouped['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int_80_score_scaled    0.548497\n",
       "int_95_score_scaled    0.959561\n",
       "dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catch22_clust_test_pred_int_df_grouped[['int_80_score_scaled', 'int_95_score_scaled']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prediction interval df to a csv file\n",
    "catch22_clust_test_pred_int_df.to_csv(\"Results/Global/LightGBM Default/Catch22 KMeans/test_pred_intervals.csv\", \n",
    "                                      index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test - TSFeat KMeans Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete variables which are no longer needed\n",
    "del catch22_clust_test_pred_int_df\n",
    "del catch22_clust_test_pred_int\n",
    "del y_actual_sub\n",
    "del catch22_clust_test_perf\n",
    "del catch22_clust_test_preds\n",
    "del catch22_clust_residuals\n",
    "del catch22_clust_mods\n",
    "del train_val_df_full_catch22_clust\n",
    "del test_df_full_catch22_clust\n",
    "del catch22_clust\n",
    "del catch22_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage collect\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in cluster assignments from tsfeat-based clusters and rename the cluster assignemnt field to 'cluster'\n",
    "tsfeat_clust = pd.read_csv(\"Results/Clustering/KMeans/kmeans_tsfeat_clustering_assign.csv\")\n",
    "tsfeat_clust['cluster'] = tsfeat_clust['kmeans_tsfeat_clust_assign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training and test data with the cluster assignments\n",
    "train_val_df_full_tsfeat_clust = train_val_df_full.merge(tsfeat_clust, on=\"ts_index\")\n",
    "test_df_full_tsfeat_clust = test_df_full.merge(tsfeat_clust, on=\"ts_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tsfeat LGBM Models: 100%|█████████████████████████| 2/2 [00:32<00:00, 16.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Parallel loop through the clusters and train a light gbm model for each clutser\n",
    "# Trained models are saved into the tsfeat_clust_mods list\n",
    "with tqdm_joblib(tqdm(desc=\"tsfeat LGBM Models\", \n",
    "                      total=len(tsfeat_clust.cluster.unique()))) as progress_bar:\n",
    "    tsfeat_clust_mods = Parallel(n_jobs=2)(delayed(train_lgbm_clust)(train_val_df_full_tsfeat_clust, \n",
    "                                                                      i, \n",
    "                                                                      lag_n\n",
    "                                                                    ) for i in range(1, len(tsfeat_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model in the list, save to a file\n",
    "for clust_no in range(1, len(tsfeat_clust.cluster.unique())+1):\n",
    "    filename = f'Results/Global/LightGBM Default/TSFeat KMeans/model_{clust_no}'\n",
    "    joblib.dump(tsfeat_clust_mods[clust_no-1], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfeat_clust_mods = list()\n",
    "\n",
    "for clust_no in range(1, len(tsfeat_clust.cluster.unique())+1):\n",
    "    filename = f'Results/Global/LightGBM Default/TSFeat KMeans/model_{clust_no}'\n",
    "    tsfeat_clust_mods.append(joblib.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tsfeat LGBM Residuals: 100%|██████████████████████| 2/2 [00:12<00:00,  6.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# In parallel, loop through the clusters and compute model residuals. The list of residuals for each model\n",
    "# is saved as an entry in the tsfeat_clust_residuals list \n",
    "with tqdm_joblib(tqdm(desc=\"tsfeat LGBM Residuals\", \n",
    "                      total=len(tsfeat_clust.cluster.unique()))) as progress_bar:\n",
    "    tsfeat_clust_residuals = Parallel(n_jobs=2)(delayed(compute_lgbm_resid_clust)(tsfeat_clust_mods[i-1],\n",
    "                                                                                  i,\n",
    "                                                                                  train_val_df_full_tsfeat_clust,\n",
    "                                                                                  lag_n\n",
    "                                                                                 ) for i in range(1, len(tsfeat_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[-10.803316036628644, 14.275223393875137, -39....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[35.3488709348197, -13.231145805264703, 16.757...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                           residual\n",
       "0        1  [-10.803316036628644, 14.275223393875137, -39....\n",
       "1        2  [35.3488709348197, -13.231145805264703, 16.757..."
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsfeat_clust_res_df = pd.DataFrame({'cluster': list({(i+1): tsfeat_clust_residuals[i] for i in range(len(tsfeat_clust_residuals))}.keys()),\n",
    "                                    'residual': list({(i+1): tsfeat_clust_residuals[i] for i in range(len(tsfeat_clust_residuals))}.values())})\n",
    "\n",
    "tsfeat_clust_res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfeat_clust_res_df.to_csv(\"Results/Global/LightGBM Default/TSFeat KMeans/residual.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tsfeat LGBM Test Preds: 100%|█████████████████████| 2/2 [00:05<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# In parallel, loop through the models and compute the set of test predictions for each cluster\n",
    "# The df of test preds for each cluster is an entry in the tsfeat_clust_test_preds list\n",
    "with tqdm_joblib(tqdm(desc=\"tsfeat LGBM Test Preds\", \n",
    "                      total=len(tsfeat_clust.cluster.unique()))) as progress_bar:\n",
    "    tsfeat_clust_test_preds = Parallel(n_jobs=2)(delayed(compute_lgbm_test_preds_clust)(tsfeat_clust_mods[i-1],\n",
    "                                                                                        i,\n",
    "                                                                                        test_df_full_tsfeat_clust,\n",
    "                                                                                        lag_n\n",
    "                                                                                       ) for i in range(1, len(tsfeat_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty data frame to save all test pred dfs into\n",
    "tsfeat_clust_test_preds_df = pd.concat(tsfeat_clust_test_preds)\n",
    "\n",
    "# With the full df of test preds, compute prediction performance\n",
    "tsfeat_clust_test_perf = compute_lgbm_test_perf(tsfeat_clust_test_preds_df,\n",
    "                                                 test_df_full_tsfeat_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scaled/normalized metrics to the data frame\n",
    "tsfeat_clust_test_perf['nrmse'] = tsfeat_clust_test_perf['rmse']/tsfeat_clust_test_perf['mean']\n",
    "tsfeat_clust_test_perf['smae'] = tsfeat_clust_test_perf['mae']/tsfeat_clust_test_perf['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmse      31.225672\n",
       "mae       20.743977\n",
       "mean     265.435072\n",
       "nrmse      0.141735\n",
       "smae       0.096075\n",
       "dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the means of the performance metrics\n",
    "tsfeat_clust_test_perf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tsfeat LGBM PI: 100%|████████████████████████████| 2/2 [21:41<00:00, 650.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# For each data frame of predictions, compute bootstrap prediction intervals\n",
    "# Save the df of pred ints for each as an entry in the list tsfeat_clust_test_pred_int\n",
    "with tqdm_joblib(tqdm(desc=\"tsfeat LGBM PI\", \n",
    "                      total=len(tsfeat_clust.cluster.unique()))) as progress_bar:\n",
    "    tsfeat_clust_test_pred_int = Parallel(n_jobs=2)(delayed(compute_lgbm_boostrap_int)(tsfeat_clust_test_preds[i-1],\n",
    "                                                                                        tsfeat_clust_residuals[i-1],\n",
    "                                                                                        1000) for i in range(1, len(tsfeat_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, add the true target values for that cluster as a column to the df of pred ints\n",
    "for n in range(1, len(tsfeat_clust_test_pred_int)+1):\n",
    "    \n",
    "    y_actual_sub = test_df_full_tsfeat_clust.query(\"cluster==@n\").copy().iloc[:,0].to_list()\n",
    "    \n",
    "    tsfeat_clust_test_pred_int[n-1]['actual'] = y_actual_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, compute 80% and 95% PI scores\n",
    "for m in range(len(tsfeat_clust_test_pred_int)):\n",
    "    tsfeat_clust_test_pred_int[m]['int_95_score'] = interval_score(tsfeat_clust_test_pred_int[m]['actual'],\n",
    "                                                                    tsfeat_clust_test_pred_int[m]['lo_95'],\n",
    "                                                                    tsfeat_clust_test_pred_int[m]['hi_95'],\n",
    "                                                                    0.95\n",
    "                                                                   )\n",
    "\n",
    "    tsfeat_clust_test_pred_int[m]['int_80_score'] = interval_score(tsfeat_clust_test_pred_int[m]['actual'],\n",
    "                                                                    tsfeat_clust_test_pred_int[m]['lo_80'],\n",
    "                                                                    tsfeat_clust_test_pred_int[m]['hi_80'],\n",
    "                                                                    0.80\n",
    "                                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all data frames of PIs into one data frame\n",
    "tsfeat_clust_test_pred_int_df = pd.concat(tsfeat_clust_test_pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ts_index         38.500000\n",
       "test_preds      265.658464\n",
       "lo_95           204.233591\n",
       "hi_95           329.589478\n",
       "lo_80           235.936280\n",
       "hi_80           296.449757\n",
       "actual          265.435072\n",
       "int_95_score    224.571612\n",
       "int_80_score    122.694939\n",
       "dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print means of PI scores\n",
    "tsfeat_clust_test_pred_int_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfeat_clust_test_pred_int_df_grouped = tsfeat_clust_test_pred_int_df.groupby(\"ts_index\")\\\n",
    ".agg({\"int_95_score\":\"mean\", \"int_80_score\":\"mean\", \"actual\": \"mean\"}).reset_index()\n",
    "\n",
    "tsfeat_clust_test_pred_int_df_grouped['int_95_score_scaled'] = tsfeat_clust_test_pred_int_df_grouped['int_95_score']/tsfeat_clust_test_pred_int_df_grouped['actual']\n",
    "tsfeat_clust_test_pred_int_df_grouped['int_80_score_scaled'] = tsfeat_clust_test_pred_int_df_grouped['int_80_score']/tsfeat_clust_test_pred_int_df_grouped['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int_80_score_scaled    0.581382\n",
       "int_95_score_scaled    1.051411\n",
       "dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsfeat_clust_test_pred_int_df_grouped[['int_80_score_scaled', 'int_95_score_scaled']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PI data frame to csv file\n",
    "tsfeat_clust_test_pred_int_df.to_csv(\"Results/Global/LightGBM Default/TSFeat KMeans/test_pred_intervals.csv\", \n",
    "                                      index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test - DTW Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete variables no longer in use\n",
    "del tsfeat_clust_test_pred_int_df\n",
    "del tsfeat_clust_test_pred_int\n",
    "del y_actual_sub\n",
    "del tsfeat_clust_test_perf\n",
    "del tsfeat_clust_test_preds\n",
    "del tsfeat_clust_residuals\n",
    "del tsfeat_clust_mods\n",
    "del train_val_df_full_tsfeat_clust\n",
    "del test_df_full_tsfeat_clust\n",
    "del tsfeat_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage collect\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dtw cluster assignments and add the column called 'cluster' as before\n",
    "dtw_clust = pd.read_csv(\"Results/Clustering/DTW/dtw_clustering_assign.csv\")\n",
    "dtw_clust['cluster'] = dtw_clust['dtw_clust_assign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the training and test data with the cluster assignments\n",
    "train_val_df_full_dtw_clust = train_val_df_full.merge(dtw_clust, on=\"ts_index\")\n",
    "test_df_full_dtw_clust = test_df_full.merge(dtw_clust, on=\"ts_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dtw LGBM Models: 100%|████████████████████████████| 2/2 [00:34<00:00, 17.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# In parallel, for each cluster, create a light gbm model and save to list\n",
    "with tqdm_joblib(tqdm(desc=\"dtw LGBM Models\", \n",
    "                      total=len(dtw_clust.cluster.unique()))) as progress_bar:\n",
    "    dtw_clust_mods = Parallel(n_jobs=2)(delayed(train_lgbm_clust)(train_val_df_full_dtw_clust, \n",
    "                                                                  i,\n",
    "                                                                  lag_n\n",
    "                                                                 ) for i in range(1, len(dtw_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write these models to files\n",
    "for clust_no in range(1, len(dtw_clust.cluster.unique())+1):\n",
    "    filename = f'Results/Global/LightGBM Default/DTW/model_{clust_no}'\n",
    "    joblib.dump(dtw_clust_mods[clust_no-1], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_clust_mods = list()\n",
    "\n",
    "for clust_no in range(1, len(dtw_clust.cluster.unique())+1):\n",
    "    filename = f'Results/Global/LightGBM Default/DTW/model_{clust_no}'\n",
    "    dtw_clust_mods.append(joblib.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dtw LGBM Residuals: 100%|█████████████████████████| 2/2 [00:12<00:00,  6.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# In parallel, loop through the models created above and compute residuals. Save the list of residuals for each\n",
    "# model to a list\n",
    "with tqdm_joblib(tqdm(desc=\"dtw LGBM Residuals\", \n",
    "                      total=len(dtw_clust.cluster.unique()))) as progress_bar:\n",
    "    dtw_clust_residuals = Parallel(n_jobs=2)(delayed(compute_lgbm_resid_clust)(dtw_clust_mods[i-1],\n",
    "                                                                               i,\n",
    "                                                                               train_val_df_full_dtw_clust,\n",
    "                                                                               lag_n\n",
    "                                                                              ) for i in range(1, len(dtw_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[6.975373456564654, 13.106412046579209, -27.45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[-3.0769047407929975, 22.192841270775546, -35....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                           residual\n",
       "0        1  [6.975373456564654, 13.106412046579209, -27.45...\n",
       "1        2  [-3.0769047407929975, 22.192841270775546, -35...."
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtw_clust_res_df = pd.DataFrame({'cluster': list({(i+1): dtw_clust_residuals[i] for i in range(len(dtw_clust_residuals))}.keys()),\n",
    "                                 'residual': list({(i+1): dtw_clust_residuals[i] for i in range(len(dtw_clust_residuals))}.values())})\n",
    "\n",
    "dtw_clust_res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_clust_res_df.to_csv(\"Results/Global/LightGBM Default/DTW/residual.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dtw LGBM Test Preds: 100%|████████████████████████| 2/2 [00:05<00:00,  2.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# For each model, in parallel, loop through and compute predictions on the test set. Save the prediction data \n",
    "# frames to a list\n",
    "with tqdm_joblib(tqdm(desc=\"dtw LGBM Test Preds\", \n",
    "                      total=len(dtw_clust.cluster.unique()))) as progress_bar:\n",
    "    dtw_clust_test_preds = Parallel(n_jobs=2)(delayed(compute_lgbm_test_preds_clust)(dtw_clust_mods[i-1],\n",
    "                                                                                     i,\n",
    "                                                                                     test_df_full_dtw_clust,\n",
    "                                                                                     lag_n\n",
    "                                                                                    ) for i in range(1, len(dtw_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty data frame to hold all test preds\n",
    "dtw_clust_test_preds_df = pd.concat(dtw_clust_test_preds)\n",
    "\n",
    "# Compute performance using this data frame of all test preds\n",
    "dtw_clust_test_perf = compute_lgbm_test_perf(dtw_clust_test_preds_df,\n",
    "                                                 test_df_full_dtw_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalized and scaled performance metrics\n",
    "dtw_clust_test_perf['nrmse'] = dtw_clust_test_perf['rmse']/dtw_clust_test_perf['mean']\n",
    "dtw_clust_test_perf['smae'] = dtw_clust_test_perf['mae']/dtw_clust_test_perf['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmse      30.910959\n",
       "mae       20.525132\n",
       "mean     265.435072\n",
       "nrmse      0.139256\n",
       "smae       0.094090\n",
       "dtype: float64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the means of the performance metrics\n",
    "dtw_clust_test_perf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dtw LGBM PI: 100%|███████████████████████████████| 2/2 [20:50<00:00, 625.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the preds and residuals for the model for each cluster (in parallel) and compute a new data frame\n",
    "# with bootstrap PIs. Save these data frames to a list\n",
    "with tqdm_joblib(tqdm(desc=\"dtw LGBM PI\", \n",
    "                      total=len(dtw_clust.cluster.unique()))) as progress_bar:\n",
    "    dtw_clust_test_pred_int = Parallel(n_jobs=2)(delayed(compute_lgbm_boostrap_int)(dtw_clust_test_preds[i-1],\n",
    "                                                                                        dtw_clust_residuals[i-1],\n",
    "                                                                                        1000) for i in range(1, len(dtw_clust.cluster.unique())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, grab the true values for the target variable and add those as a column to the PI data frame\n",
    "for n in range(1, len(dtw_clust_test_pred_int)+1):\n",
    "    \n",
    "    y_actual_sub = test_df_full_dtw_clust.query(\"cluster==@n\").copy().iloc[:,0].to_list()\n",
    "    \n",
    "    dtw_clust_test_pred_int[n-1]['actual'] = y_actual_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster's PI df, compute the 95% and 80% PI scores\n",
    "for m in range(len(dtw_clust_test_pred_int)):\n",
    "    dtw_clust_test_pred_int[m]['int_95_score'] = interval_score(dtw_clust_test_pred_int[m]['actual'],\n",
    "                                                                    dtw_clust_test_pred_int[m]['lo_95'],\n",
    "                                                                    dtw_clust_test_pred_int[m]['hi_95'],\n",
    "                                                                    0.95\n",
    "                                                                   )\n",
    "\n",
    "    dtw_clust_test_pred_int[m]['int_80_score'] = interval_score(dtw_clust_test_pred_int[m]['actual'],\n",
    "                                                                    dtw_clust_test_pred_int[m]['lo_80'],\n",
    "                                                                    dtw_clust_test_pred_int[m]['hi_80'],\n",
    "                                                                    0.80\n",
    "                                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the list of PI data frames and append to one data frame\n",
    "dtw_clust_test_pred_int_df = pd.concat(dtw_clust_test_pred_int)\n",
    "# for pred_int_df_clust in dtw_clust_test_pred_int:\n",
    "#     dtw_clust_test_pred_int_df = dtw_clust_test_pred_int_df.append(pred_int_df_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ts_index         38.500000\n",
       "test_preds      265.789873\n",
       "lo_95           207.626934\n",
       "hi_95           326.228252\n",
       "lo_80           235.786707\n",
       "hi_80           296.796925\n",
       "actual          265.435072\n",
       "int_95_score    200.024829\n",
       "int_80_score    115.749048\n",
       "dtype: float64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print means of PI scores\n",
    "dtw_clust_test_pred_int_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_clust_test_pred_int_df_grouped = dtw_clust_test_pred_int_df.groupby(\"ts_index\")\\\n",
    ".agg({'int_95_score':'mean', 'int_80_score':'mean', 'actual':'mean'}).reset_index()\n",
    "\n",
    "dtw_clust_test_pred_int_df_grouped['int_95_score_scaled'] = dtw_clust_test_pred_int_df_grouped['int_95_score']/dtw_clust_test_pred_int_df_grouped['actual']\n",
    "dtw_clust_test_pred_int_df_grouped['int_80_score_scaled'] = dtw_clust_test_pred_int_df_grouped['int_80_score']/dtw_clust_test_pred_int_df_grouped['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int_80_score_scaled    0.554662\n",
       "int_95_score_scaled    0.953065\n",
       "dtype: float64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtw_clust_test_pred_int_df_grouped[['int_80_score_scaled', 'int_95_score_scaled']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df of PIs to a csv file\n",
    "dtw_clust_test_pred_int_df.to_csv(\"Results/Global/LightGBM Default/DTW/test_pred_intervals.csv\", \n",
    "                                      index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
