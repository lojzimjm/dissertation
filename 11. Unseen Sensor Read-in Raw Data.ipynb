{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284d72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the libraries are not yet installed, they can be installed using conda commands similar to the below\n",
    "# %conda install numpy\n",
    "# %conda install pandas\n",
    "# %conda install pandasql \n",
    "# %conda install openpyxl\n",
    "\n",
    "# Something like the following may also work if the above does not\n",
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} numpy\n",
    "# !conda install --yes --prefix {sys.prefix} pandas\n",
    "# !conda install --yes --prefix {sys.prefix} pandasql\n",
    "# !conda install --yes --prefix {sys.prefix} openpyxl\n",
    "\n",
    "# To install a specific version, add the version to the install command\n",
    "# E.g., %conda install numpy=1.20.3\n",
    "\n",
    "# If all else fails, use pip or follow additional advice such as found at\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "\n",
    "# If your plan to use pip (especially if you are not working within a specified conda environment), \n",
    "# the pip commands might look like:\n",
    "# pip install numpy\n",
    "# pip install pandas\n",
    "# pip install pandasql\n",
    "# pip install open pyxl\n",
    "\n",
    "# To install a specific version, add the version to the pip install command\n",
    "# E.g., pip install numpy==1.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd68b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandasql as psql\n",
    "import glob\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d15053cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create intermediate and processed directories for later use\n",
    "if not os.path.exists(\"Data/Unseen Sensor/Intermediate\"):\n",
    "    os.mkdir(\"Data/Unseen Sensor/Intermediate\")\n",
    "\n",
    "if not os.path.exists(\"Data/Unseen Sensor/Processed\"):\n",
    "    os.mkdir(\"Data/Unseen Sensor/Processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76eb024",
   "metadata": {},
   "source": [
    "# Functions to Read in Raw Data and Impute with Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f722729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_time(df, interval=\"15min\", min_date=None, max_date=None):\n",
    "    \"\"\"Function to generate full df of dates to determine which ones have missing data\"\"\"\n",
    "    \n",
    "    # Specify the start and end timestamps for the full date range\n",
    "    if not min_date:\n",
    "        start_date = df[\"timestamp\"].min()\n",
    "    else:\n",
    "        start_date = min_date\n",
    "    \n",
    "    if not max_date:\n",
    "        end_date = df[\"timestamp\"].max()\n",
    "    else:\n",
    "        end_date = max_date\n",
    "    \n",
    "    # Create a full date range using specified freq, typically either 15min or 5 min depending on the data \n",
    "    date_list = pd.date_range(start=start_date, end=end_date, freq=interval)\n",
    "    \n",
    "    # Put this date range into a df\n",
    "    date_df = pd.DataFrame({\"timestamp\": date_list})\n",
    "    \n",
    "    # Create columns for date, day of week, and day of year in addition to the timestamp column\n",
    "    date_df[\"date\"] = pd.to_datetime(date_df[\"timestamp\"].astype(\"string\").str[:10])\n",
    "    date_df[\"day_of_week\"] = date_df[\"date\"].dt.dayofweek\n",
    "    date_df[\"day_of_year\"] = date_df[\"date\"].dt.dayofyear\n",
    "    \n",
    "    return date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6759c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_highways_england(fname, min_date=None, max_date=None):\n",
    "    \"\"\"Function to read in csv file of highway sensor data\"\"\"\n",
    "    \n",
    "    # Read file into Pandas df\n",
    "    df = pd.read_csv(fname)\n",
    "    \n",
    "    # Grab relevant columns from df\n",
    "    df = df[[\"Site Name\", \"Report Date\", \"Time Period Ending\", \"Time Interval\", \"Avg mph\", \"Total Volume\"]]\n",
    "    \n",
    "    # Re-format date field and cast to string\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Report Date\"], format='%d/%m/%Y 00:00:00').astype(\"string\") \n",
    "    \n",
    "    # Grab the timestamp of the time-period in the hour\n",
    "    df[\"Time Period Ending\"] = df[\"Time Period Ending\"].astype(\"string\")\n",
    "    \n",
    "    # Create a true timestamp which includes both date and hour and minutes\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time Period Ending\"])\n",
    "    \n",
    "    # Subset columns and rename to include _ to make columns easier to work with\n",
    "    df = df[[\"Site Name\", \"Timestamp\", \"Time Interval\", \"Avg mph\", \"Total Volume\"]]\\\n",
    "    .rename(columns={\"Site Name\": \"site_name\",\n",
    "                     \"Timestamp\":\"timestamp\",\n",
    "                     \"Time Interval\": \"interval_of_day\",\n",
    "                     \"Avg mph\": \"avg_mph\",\n",
    "                     \"Total Volume\": \"total_volume\"})\n",
    "    \n",
    "    # Compute dates for left join \n",
    "    dates = full_time(df, interval=\"15min\", min_date=min_date, max_date=max_date)\n",
    "    \n",
    "    # Merge full date list with actual data\n",
    "    df = dates.merge(df, how=\"left\", on=\"timestamp\")\n",
    "    \n",
    "    site = df[\"site_name\"].unique()[0]\n",
    "    \n",
    "    df.fillna({\"site_name\": site}, inplace=True)\n",
    "    \n",
    "    # Use pandasql to impute the 'interval_of_day' field \n",
    "    interval_of_day_impute = \"\"\"\n",
    "    SELECT site_name,\n",
    "           day_of_week,\n",
    "           date(date) AS date,\n",
    "           day_of_year,\n",
    "           timestamp,\n",
    "           DENSE_RANK() OVER (PARTITION BY DATE ORDER BY timestamp) - 1 AS interval_of_day,\n",
    "           avg_mph,\n",
    "           total_volume\n",
    "    FROM df\n",
    "    \"\"\"\n",
    "    df = psql.sqldf(interval_of_day_impute, locals())\n",
    "    \n",
    "    # Create field with T/F if speed data is missing\n",
    "    df[\"missing_speed\"] = np.where(df[\"avg_mph\"].isnull(), True, False)\n",
    "    \n",
    "    # Create field with T/F if volume data is missing\n",
    "    df[\"missing_volume\"] = np.where(df[\"total_volume\"].isnull(), True, False)\n",
    "\n",
    "    # Set DateTime Index\n",
    "#     df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "#     df = df.set_index(\"timestamp\")\n",
    "    \n",
    "    # Use linear interoplation to fill in nulls\n",
    "    df[\"avg_mph\"] = df[\"avg_mph\"].interpolate()\n",
    "    df[\"total_volume\"] = df[\"total_volume\"].interpolate()\n",
    "    \n",
    "#     df = df.reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65fb858",
   "metadata": {},
   "source": [
    "# Read in Raw Data, Impute, and Write to New Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a1422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the min and max dates for the 5-min and 15-min data\n",
    "min_date_5 = \"2019-01-01 00:04:00\"\n",
    "min_date_15 = \"2019-01-01 00:14:00\"\n",
    "max_date = \"2019-12-31 23:59:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034a8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b478a5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data/Unseen Sensor/Raw/A19-9336-1_Northbound_2019.csv\n",
      "Reading Data/Unseen Sensor/Raw/A66-9521-1_Westbound.csv\n",
      "Reading Data/Unseen Sensor/Raw/M40-7048-2_Southbound.csv\n",
      "Reading Data/Unseen Sensor/Raw/M62-2056A_Eastbound.csv\n"
     ]
    }
   ],
   "source": [
    "# Read in all raw highways england data files, impute, and write to the intermediate direcotry of the data folder\n",
    "for fname in glob.glob(\"Data/Unseen Sensor/Raw/*.csv\"):\n",
    "    print(\"Reading {}\".format(fname))\n",
    "    \n",
    "    fname_new = \"Data/Unseen Sensor/Intermediate/{}_Intermediate.csv\".format(fname.split(\"/\")[-1].split(\".\")[0])\n",
    "    \n",
    "    df = read_highways_england(fname, min_date_15, max_date)\n",
    "    \n",
    "    df.to_csv(fname_new, index=False)\n",
    "    \n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b8184c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the counter\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0271aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
